{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPORT LIBRARY**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "B_tqXBlYI4n4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GRUP 1: IMPORT LIBRARY DAN SETUP PATH ---\n",
        "\n",
        "# === Cell 1: Verifikasi Versi Python ===\n",
        "print(\"--- GRUP 1: SEL 1 ---\")\n",
        "!python --version\n",
        "print(\"Verifikasi versi Python selesai.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRYYlOv7djm7",
        "outputId": "d4dcd9f5-26a6-4499-f008-f82802787d25"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GRUP 1: SEL 1 ---\n",
            "Python 3.11.12\n",
            "Verifikasi versi Python selesai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Instalasi Library yang Diperlukan ===\n",
        "# (Sesuai daftar Anda, tanpa TensorFlow dan matplotlib)\n",
        "print(\"--- GRUP 1: SEL 2 ---\")\n",
        "!pip install pandas numpy gensim Sastrawi nltk scikit-learn scipy\n",
        "print(\"Instalasi library yang diperlukan (jika ada) selesai.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vt_QyT9EdlRT",
        "outputId": "5db0a736-6d75-4e3f-dc89-ed05758dea17"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GRUP 1: SEL 2 ---\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Instalasi library yang diperlukan (jika ada) selesai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Import Library ===\n",
        "print(\"--- GRUP 1: SEL 3 ---\")\n",
        "# Library Python Standar\n",
        "import math\n",
        "import re\n",
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import pickle # Untuk menyimpan/memuat objek Python\n",
        "import logging # Untuk logging\n",
        "from datetime import datetime # Untuk timestamp\n",
        "import warnings # Untuk menangani peringatan\n",
        "\n",
        "# Library Eksternal Utama (sesuai daftar Anda)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors, FastText # Tetap menggunakan FastText untuk training\n",
        "from gensim.models.callbacks import CallbackAny2Vec\n",
        "\n",
        "# Untuk Text Preprocessing\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "# import nltk # NLTK tidak diimpor jika hanya stopwords Sastrawi\n",
        "# Jika NLTK stopwords juga diperlukan:\n",
        "# nltk.download('stopwords', quiet=True)\n",
        "# from nltk.corpus import stopwords as nltk_stopwords\n",
        "\n",
        "# Untuk Sklearn Utilities\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine_similarity # Digunakan di referensi LSTM Anda\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# Untuk Statistik (Korelasi)\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "\n",
        "# Untuk Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"Import library selesai.\\n\")\n",
        "\n",
        "# Konfigurasi dasar untuk logging (opsional, bisa disesuaikan)\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "warnings.filterwarnings('ignore', category=UserWarning) # Bisa lebih spesifik jika perlu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5y-VmowHdo3O",
        "outputId": "1c2d253a-5caf-4ed5-ef37-7f217dbcda47"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GRUP 1: SEL 3 ---\n",
            "Import library selesai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Verifikasi Versi Library Utama ===\n",
        "print(\"--- GRUP 1: SEL 4 ---\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Gensim version: {gensim.__version__}\")\n",
        "\n",
        "# Import pkg_resources to get package version\n",
        "import pkg_resources\n",
        "try:\n",
        "    sastrawi_version = pkg_resources.get_distribution(\"Sastrawi\").version\n",
        "    print(f\"Sastrawi version: {sastrawi_version}\")\n",
        "except pkg_resources.DistributionNotFound:\n",
        "    print(\"Sastrawi version: Not found (package not installed correctly?)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error getting Sastrawi version: {e}\")\n",
        "\n",
        "\n",
        "import sklearn\n",
        "print(f\"Scikit-learn version: {sklearn.__version__}\")\n",
        "import scipy\n",
        "print(f\"SciPy version: {scipy.__version__}\")\n",
        "print(\"Verifikasi versi library selesai.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81YT8GF9dqki",
        "outputId": "0ad8cf68-f9de-4811-cb9c-006c1085ad5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GRUP 1: SEL 4 ---\n",
            "NumPy version: 1.26.4\n",
            "Pandas version: 2.2.2\n",
            "Gensim version: 4.3.3\n",
            "Sastrawi version: 1.0.1\n",
            "Scikit-learn version: 1.6.1\n",
            "SciPy version: 1.13.1\n",
            "Verifikasi versi library selesai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Mount Google Drive dan Definisikan/Buat Path ===\n",
        "print(\"--- GRUP 1: SEL 5 ---\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive berhasil di-mount.\")\n",
        "\n",
        "    # Definisikan direktori dasar proyek (sesuai permintaan terakhir Anda)\n",
        "    BASE_PROJECT_DIR = '/content/drive/MyDrive/Model/'\n",
        "\n",
        "    # Definisikan path untuk setiap direktori utama sesuai struktur Anda\n",
        "    DATASET_DIR = os.path.join(BASE_PROJECT_DIR, '01_Dataset/')\n",
        "    PREPROCESSING_DIR = os.path.join(BASE_PROJECT_DIR, '02_Preprocessing_Artifacts/')\n",
        "    EMBEDDING_DIR = os.path.join(BASE_PROJECT_DIR, '03_Word_Embeddings/')\n",
        "    LSTM_MANUAL_MODEL_DIR = os.path.join(BASE_PROJECT_DIR, '04_LSTM_Model/') # Disesuaikan dengan nama folder Anda\n",
        "    LOGS_DIR = os.path.join(BASE_PROJECT_DIR, '05_Training_Logs/')\n",
        "\n",
        "    # Path spesifik untuk subfolder di dalam 03_Word_Embeddings\n",
        "    PRETRAINED_EMBEDDING_DIR = os.path.join(EMBEDDING_DIR, 'Pretrained/')\n",
        "    FINE_TUNED_EMBEDDING_DIR = os.path.join(EMBEDDING_DIR, 'Fine_Tuned/')\n",
        "    EMBEDDING_BEST_KV_SUBDIR = os.path.join(FINE_TUNED_EMBEDDING_DIR, 'embedding_best_kv/')\n",
        "    EMBEDDING_FINAL_MODEL_SUBDIR = os.path.join(FINE_TUNED_EMBEDDING_DIR, 'embedding_final_model/')\n",
        "\n",
        "    # Path untuk file-file spesifik yang akan digunakan/dihasilkan\n",
        "    # Dataset\n",
        "    dataset_path_global = os.path.join(DATASET_DIR, 'Semua_Soal.json')\n",
        "    input_soal_txt_path_global = os.path.join(DATASET_DIR, 'input_soal.txt')\n",
        "\n",
        "    # Preprocessing Artifacts\n",
        "    tokenizer_lstm_path_global = os.path.join(PREPROCESSING_DIR, 'tokenizer_lstm.pkl') # Meskipun LSTM manual mungkin tidak pakai ini\n",
        "\n",
        "    # Embedding Pra-latih\n",
        "    pretrained_embedding_path_global = os.path.join(PRETRAINED_EMBEDDING_DIR, 'cc.id.300.vec')\n",
        "\n",
        "    # Embedding Fine-Tuned (akan dinamai \"glove_\" tapi dilatih dengan FastText)\n",
        "    embedding_training_metadata_path_global = os.path.join(FINE_TUNED_EMBEDDING_DIR, 'embedding_training_metadata.json')\n",
        "    embedding_best_kv_file_path_global = os.path.join(EMBEDDING_BEST_KV_SUBDIR, 'glove_best_similarity_keyedvectors.kv')\n",
        "    # Path untuk model FastText utama yang akan kita anggap sebagai \"glove_finetuned\"\n",
        "    glove_finetuned_model_path_global = os.path.join(EMBEDDING_FINAL_MODEL_SUBDIR, 'glove_finetuned.model')\n",
        "    # Path untuk model FastText final yang mungkin disimpan oleh logger Gensim\n",
        "    glove_final_model_internal_path_global = os.path.join(EMBEDDING_FINAL_MODEL_SUBDIR, 'glove_final_model.model')\n",
        "    # File .npy akan dibuat oleh gensim saat menyimpan model FastText jika ada ngrams\n",
        "    # glove_final_model_vectors_ngrams_path = os.path.join(EMBEDDING_FINAL_MODEL_SUBDIR, 'glove_final_model.model.wv.vectors_ngrams.npy')\n",
        "    # glove_finetuned_vectors_ngrams_path = os.path.join(EMBEDDING_FINAL_MODEL_SUBDIR, 'glove_finetuned.model.wv.vectors_ngrams.npy')\n",
        "    mean_fasttext_vector_file_path_global = os.path.join(FINE_TUNED_EMBEDDING_DIR, 'mean_fasttext_vector.npy') # Dipindah ke sini\n",
        "\n",
        "    # Model LSTM (Manual NumPy) dan Metadatanya\n",
        "    # Di *notebook* asli Anda, path LSTM adalah 'final_lstm_model.h5'. Karena kita pakai NumPy, kita ganti ke .npz\n",
        "    # dan metadata .json-nya.\n",
        "    lstm_manual_model_file_path_global = os.path.join(LSTM_MANUAL_MODEL_DIR, 'final_lstm_model.npz') # Sesuai referensi LSTM manual, tapi nama file dari Anda\n",
        "    lstm_manual_metadata_file_path_global = os.path.join(LSTM_MANUAL_MODEL_DIR, 'model_metadata.json') # Sesuai struktur Anda\n",
        "\n",
        "    # Log Training\n",
        "    embedding_training_log_file_global = os.path.join(LOGS_DIR, 'training_log.json')\n",
        "    lstm_training_log_file_global = os.path.join(LOGS_DIR, 'training_lstm_log.json') # Mungkin tidak relevan untuk LSTM manual\n",
        "    hasil_evaluasi_akhir_file_global = os.path.join(LOGS_DIR, 'hasil_evaluasi_akhir.json')\n",
        "\n",
        "\n",
        "    # List semua direktori yang perlu dibuat\n",
        "    directories_to_create = [\n",
        "        BASE_PROJECT_DIR, DATASET_DIR, PREPROCESSING_DIR,\n",
        "        PRETRAINED_EMBEDDING_DIR, FINE_TUNED_EMBEDDING_DIR,\n",
        "        EMBEDDING_BEST_KV_SUBDIR, EMBEDDING_FINAL_MODEL_SUBDIR,\n",
        "        LSTM_MANUAL_MODEL_DIR, LOGS_DIR\n",
        "    ]\n",
        "\n",
        "    # Buat semua direktori jika belum ada\n",
        "    for path_dir in directories_to_create:\n",
        "        os.makedirs(path_dir, exist_ok=True)\n",
        "        # print(f\"Memastikan direktori ada atau telah dibuat: {path_dir}\") # Komentari agar output tidak terlalu verbose\n",
        "\n",
        "    print(f\"\\nBASE_PROJECT_DIR telah diatur ke: {BASE_PROJECT_DIR}\")\n",
        "    print(\"Path direktori dan file utama telah didefinisikan.\")\n",
        "\n",
        "    # Verifikasi opsional untuk file penting\n",
        "    essential_files_check = {\n",
        "        \"Dataset Utama\": dataset_path_global,\n",
        "        \"Input Soal Txt\": input_soal_txt_path_global,\n",
        "        \"Embedding Pra-latih (cc.id.300.vec)\": pretrained_embedding_path_global\n",
        "    }\n",
        "    for desc, file_path_check in essential_files_check.items():\n",
        "        if not os.path.exists(file_path_check):\n",
        "            print(f\"PERINGATAN: File esensial '{desc}' TIDAK ditemukan di '{file_path_check}'. Pastikan file ada.\")\n",
        "        else:\n",
        "            print(f\"File esensial '{desc}' ditemukan di '{file_path_check}'.\")\n",
        "\n",
        "    print(\"\\nSetup path selesai.\\n\")\n",
        "\n",
        "    # Perintah untuk mengecek isi MyDrive dan BASE_PROJECT_DIR (opsional, untuk debugging)\n",
        "    # print(\"\\nIsi dari MyDrive Anda:\")\n",
        "    # !ls \"/content/drive/MyDrive/\"\n",
        "    # print(f\"\\nIsi dari BASE_PROJECT_DIR ({BASE_PROJECT_DIR}):\")\n",
        "    # !ls \"{BASE_PROJECT_DIR}\"\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error saat mounting drive atau setup path: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-rgH6axdsz7",
        "outputId": "dbd60cc5-6ad6-45fd-847e-88802be5f357"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GRUP 1: SEL 5 ---\n",
            "Mounted at /content/drive\n",
            "Google Drive berhasil di-mount.\n",
            "\n",
            "BASE_PROJECT_DIR telah diatur ke: /content/drive/MyDrive/Model/\n",
            "Path direktori dan file utama telah didefinisikan.\n",
            "File esensial 'Dataset Utama' ditemukan di '/content/drive/MyDrive/Model/01_Dataset/Semua_Soal.json'.\n",
            "PERINGATAN: File esensial 'Input Soal Txt' TIDAK ditemukan di '/content/drive/MyDrive/Model/01_Dataset/input_soal.txt'. Pastikan file ada.\n",
            "PERINGATAN: File esensial 'Embedding Pra-latih (cc.id.300.vec)' TIDAK ditemukan di '/content/drive/MyDrive/Model/03_Word_Embeddings/Pretrained/cc.id.300.vec'. Pastikan file ada.\n",
            "\n",
            "Setup path selesai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Inisialisasi Global Variabel dan Sastrawi ===\n",
        "# (Bagian dari referensi Cell 5 LSTM manual Anda dan setup Sastrawi)\n",
        "print(\"--- GRUP 1: SEL 6 ---\")\n",
        "\n",
        "# Variabel Global untuk model dan data (akan diisi nanti)\n",
        "ft_model_global = None # Model FastText hasil fine-tuning (akan dinamai seolah GloVe)\n",
        "mean_fasttext_vector_global = np.zeros(300) # Default, akan di-load atau dihitung dari ft_model_global\n",
        "embedding_model_large_vocab_global = None # Untuk cc.id.300.vec (pra-latih besar)\n",
        "mean_vector_for_large_vocab_global = np.zeros(300) # Untuk cc.id.300.vec\n",
        "lstm_manual_model_instance_global = None # Instance dari kelas LSTM manual\n",
        "\n",
        "# Inisialisasi Sastrawi (jika belum diinisialisasi atau ingin memastikan)\n",
        "try:\n",
        "    stemmer_factory = StemmerFactory()\n",
        "    stemmer_global = stemmer_factory.create_stemmer()\n",
        "    stopword_remover_factory = StopWordRemoverFactory()\n",
        "    stopwords_sastrawi_global = stopword_remover_factory.get_stop_words()\n",
        "    active_stopwords_list_global = stopwords_sastrawi_global # Bisa ditambahkan custom stopwords di sini jika perlu\n",
        "    print(\"Inisialisasi Sastrawi stemmer dan stopword remover berhasil.\")\n",
        "    print(f\"Menggunakan daftar stopwords dari Sastrawi ({len(active_stopwords_list_global)} kata).\")\n",
        "except Exception as e:\n",
        "    print(f\"Gagal menginisialisasi Sastrawi: {e}\")\n",
        "    stemmer_global = None\n",
        "    active_stopwords_list_global = []\n",
        "\n",
        "# Variabel lain dari referensi LSTM manual Anda (jika relevan secara global)\n",
        "# LSTM_INPUT_DIM_GLOBAL = 300 # Akan ditentukan dari vector_size embedding\n",
        "# LSTM_HIDDEN_DIM_GLOBAL = 128 # Bisa diatur sebagai parameter\n",
        "# LSTM_OUTPUT_DIM_GLOBAL = 1\n",
        "# LSTM_LEARNING_RATE_GLOBAL = 0.01\n",
        "\n",
        "print(\"Inisialisasi variabel global dan Sastrawi selesai.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yilrOmBzduW9",
        "outputId": "635ef8f9-0773-4e87-9931-a3e0586e6895"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- GRUP 1: SEL 6 ---\n",
            "Inisialisasi Sastrawi stemmer dan stopword remover berhasil.\n",
            "Menggunakan daftar stopwords dari Sastrawi (126 kata).\n",
            "Inisialisasi variabel global dan Sastrawi selesai.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EMBEDDING TEKS**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2AlwW5RRKOcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GRUP 2: EMBEDDING TEKS ---\n",
        "\n",
        "# === Cell 1: Inisialisasi Fungsi Preprocessing dan Similarity ===\n",
        "print(\"--- GRUP 2: SEL 1 ---\")\n",
        "\n",
        "# Pastikan stemmer_global dan active_stopwords_list_global sudah ada dari Grup 1\n",
        "if 'stemmer_global' not in globals() or 'active_stopwords_list_global' not in globals():\n",
        "    print(\"PERINGATAN: Objek Sastrawi (stemmer/stopwords) tidak ditemukan secara global. Inisialisasi ulang...\")\n",
        "    try:\n",
        "        stemmer_factory = StemmerFactory()\n",
        "        stemmer_global = stemmer_factory.create_stemmer()\n",
        "        stopword_remover_factory = StopWordRemoverFactory()\n",
        "        stopwords_sastrawi_global = stopword_remover_factory.get_stop_words()\n",
        "        active_stopwords_list_global = stopwords_sastrawi_global\n",
        "        print(\"Inisialisasi ulang Sastrawi berhasil.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal inisialisasi ulang Sastrawi: {e}\")\n",
        "        # Fallback jika Sastrawi gagal total\n",
        "        stemmer_global = type('obj', (object,), {'stem': lambda x: x})() # Stemmer dummy\n",
        "        active_stopwords_list_global = []\n",
        "\n",
        "\n",
        "def preprocess_text(text_input):\n",
        "    \"\"\"Membersihkan dan memproses teks: lowercase, hapus non-alfanumerik, tokenisasi, hapus stopwords, stemming.\"\"\"\n",
        "    if not isinstance(text_input, str):\n",
        "        return []\n",
        "    text = text_input.lower()\n",
        "    text = re.sub(r'[^a-z0-9\\s]', '', text) # Hanya biarkan huruf, angka, dan spasi\n",
        "    words = text.split()\n",
        "\n",
        "    # Gunakan active_stopwords_list_global dan stemmer_global\n",
        "    # Cek apakah stemmer_global None atau merupakan stemmer dummy\n",
        "    if stemmer_global is None or not hasattr(stemmer_global, 'stem') or stemmer_global.stem.__qualname__ == 'type.stem': # Cek stemmer dummy\n",
        "         # print(\"Peringatan: Stemmer tidak tersedia atau dummy, stemming dilewati.\")\n",
        "         words_processed = [word for word in words if word not in active_stopwords_list_global and word.strip()]\n",
        "    else:\n",
        "        words_processed = [stemmer_global.stem(word) for word in words if word not in active_stopwords_list_global and word.strip()]\n",
        "    return words_processed\n",
        "\n",
        "def calculate_cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Menghitung cosine similarity antara dua vektor numpy.\"\"\"\n",
        "    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray) or vec1.size == 0 or vec2.size == 0:\n",
        "        return 0.0\n",
        "    vec1 = vec1.flatten()\n",
        "    vec2 = vec2.flatten()\n",
        "    if vec1.shape != vec2.shape: # Pastikan bentuknya sama setelah flatten\n",
        "        # print(f\"Peringatan Cosine Sim: Bentuk vektor tidak cocok setelah flatten. vec1: {vec1.shape}, vec2: {vec2.shape}\")\n",
        "        return 0.0\n",
        "\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return 0.0\n",
        "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "    return similarity\n",
        "\n",
        "def text_to_vector_for_similarity(text_input, keyed_vectors_model, default_vector_value=None):\n",
        "    \"\"\"\n",
        "    Mengonversi teks menjadi vektor rata-rata menggunakan model KeyedVectors.\n",
        "    Menggunakan default_vector_value jika tidak ada kata yang ditemukan atau model tidak valid.\n",
        "    \"\"\"\n",
        "    # Default vector harus disediakan dan memiliki dimensi yang benar\n",
        "    if default_vector_value is None:\n",
        "        # print(\"Peringatan (text_to_vector_for_similarity): default_vector_value tidak disediakan.\")\n",
        "        if keyed_vectors_model is not None and hasattr(keyed_vectors_model, 'vector_size'):\n",
        "            default_vector_value = np.zeros(keyed_vectors_model.vector_size)\n",
        "        else: # Kasus darurat jika model juga tidak ada\n",
        "            default_vector_value = np.zeros(300) # Asumsi dimensi default jika semua gagal\n",
        "\n",
        "    if keyed_vectors_model is None or not hasattr(keyed_vectors_model, 'vector_size'):\n",
        "        # print(\"  Peringatan (text_to_vector_for_similarity): Model KeyedVectors tidak valid atau tidak dimuat.\")\n",
        "        return default_vector_value.copy() if isinstance(default_vector_value, np.ndarray) else np.zeros_like(default_vector_value)\n",
        "\n",
        "\n",
        "    if not isinstance(text_input, str) or not text_input.strip():\n",
        "        return default_vector_value.copy()\n",
        "\n",
        "    words = preprocess_text(text_input)\n",
        "    word_vectors_list = []\n",
        "    if words:\n",
        "        for word in words:\n",
        "            if word in keyed_vectors_model: # Untuk KeyedVectors, cek langsung\n",
        "                word_vectors_list.append(keyed_vectors_model[word])\n",
        "            elif hasattr(keyed_vectors_model, 'wv') and word in keyed_vectors_model.wv: # Untuk model FastText penuh\n",
        "                word_vectors_list.append(keyed_vectors_model.wv[word])\n",
        "\n",
        "\n",
        "    if not word_vectors_list:\n",
        "        return default_vector_value.copy()\n",
        "\n",
        "    return np.mean(word_vectors_list, axis=0)\n",
        "\n",
        "print(\"Fungsi preprocessing dan similarity telah didefinisikan.\\n\")"
      ],
      "metadata": {
        "id": "btzmzMH8KSDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Memuat Dataset dan Mempersiapkan Korpus untuk Fine-Tuning Embedding ===\n",
        "print(\"--- GRUP 2: SEL 2 ---\")\n",
        "list_of_all_texts_for_embedding = []\n",
        "corpus_for_embedding_fine_tuning = []\n",
        "dataset_embedding_list_g2 = [] # g2 untuk menandakan ini dari Grup 2\n",
        "\n",
        "# Pastikan dataset_path_global sudah didefinisikan di Grup 1\n",
        "if 'dataset_path_global' not in globals() or not os.path.exists(dataset_path_global):\n",
        "    print(f\"Error: File dataset '{dataset_path_global}' tidak ditemukan. Lewati persiapan korpus.\")\n",
        "else:\n",
        "    try:\n",
        "        with open(dataset_path_global, 'r', encoding='utf-8') as f:\n",
        "            dataset_embedding_list_g2 = json.load(f)\n",
        "        print(f\"Dataset untuk embedding berhasil dimuat dari '{dataset_path_global}' ({len(dataset_embedding_list_g2)} baris).\")\n",
        "\n",
        "        print(\"Mempersiapkan korpus teks untuk fine-tuning embedding...\")\n",
        "        for item_data in dataset_embedding_list_g2:\n",
        "            pertanyaan = item_data.get('pertanyaan', \"\")\n",
        "            jawaban_siswa = item_data.get('jawaban_siswa', \"\")\n",
        "            kata_kunci = item_data.get('kata_kunci', [])\n",
        "\n",
        "            if isinstance(pertanyaan, str) and pertanyaan.strip():\n",
        "                list_of_all_texts_for_embedding.append(pertanyaan)\n",
        "            if isinstance(jawaban_siswa, str) and jawaban_siswa.strip():\n",
        "                list_of_all_texts_for_embedding.append(jawaban_siswa)\n",
        "            if isinstance(kata_kunci, list):\n",
        "                for kw in kata_kunci:\n",
        "                    if isinstance(kw, str) and kw.strip():\n",
        "                        list_of_all_texts_for_embedding.append(kw)\n",
        "            elif isinstance(kata_kunci, str) and kata_kunci.strip(): # Jika kata kunci adalah string tunggal\n",
        "                 list_of_all_texts_for_embedding.append(kata_kunci)\n",
        "\n",
        "\n",
        "        # Preprocess semua teks dalam korpus ini\n",
        "        # Hasilnya akan menjadi list of list of tokens\n",
        "        if list_of_all_texts_for_embedding:\n",
        "            corpus_for_embedding_fine_tuning = [preprocess_text(text) for text in list_of_all_texts_for_embedding]\n",
        "            corpus_for_embedding_fine_tuning = [s for s in corpus_for_embedding_fine_tuning if s] # Hapus list token yang kosong\n",
        "        else:\n",
        "            print(\"Tidak ada teks yang valid untuk membentuk korpus embedding.\")\n",
        "\n",
        "\n",
        "        if not corpus_for_embedding_fine_tuning:\n",
        "            print(\"PERINGATAN: Korpus 'corpus_for_embedding_fine_tuning' kosong setelah preprocessing. Cek dataset Anda.\")\n",
        "        else:\n",
        "            print(f\"Total dokumen/teks dalam korpus yang sudah diproses untuk fine-tuning: {len(corpus_for_embedding_fine_tuning)}\")\n",
        "            if corpus_for_embedding_fine_tuning:\n",
        "                print(f\"Contoh item pertama dalam korpus (sudah di-preprocess): {corpus_for_embedding_fine_tuning[0][:10]}...\") # Tampilkan 10 token pertama\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat memuat atau memproses dataset untuk embedding: {e}\")\n",
        "\n",
        "print(\"Persiapan korpus untuk embedding selesai.\\n\")"
      ],
      "metadata": {
        "id": "kOUOQzc1QOsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Contoh Preprocessing Teks ===\n",
        "print(\"--- GRUP 2: SEL 3 ---\")\n",
        "if list_of_all_texts_for_embedding:\n",
        "    original_text_for_sample_g2 = list_of_all_texts_for_embedding[0]\n",
        "    print(f\"Contoh teks asli sebelum preprocessing: '{original_text_for_sample_g2}'\")\n",
        "    processed_sample_display_g2 = preprocess_text(original_text_for_sample_g2)\n",
        "    print(f\"Contoh teks setelah preprocessing (untuk display): {processed_sample_display_g2}\")\n",
        "else:\n",
        "    print(\"Korpus kosong, tidak bisa menampilkan contoh preprocessing.\")\n",
        "print(\"Contoh preprocessing selesai ditampilkan.\\n\")"
      ],
      "metadata": {
        "id": "Y6jdrZoJeBl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Memuat Model Embedding Pra-latih Besar (cc.id.300.vec) ===\n",
        "print(\"--- GRUP 2: SEL 4 ---\")\n",
        "# embedding_model_large_vocab_global dan mean_vector_for_large_vocab_global diinisialisasi di Grup 1\n",
        "\n",
        "# Pastikan pretrained_embedding_path_global sudah ada\n",
        "if 'pretrained_embedding_path_global' not in globals() or not os.path.exists(pretrained_embedding_path_global):\n",
        "    print(f\"Error: Path untuk embedding pra-latih '{pretrained_embedding_path_global}' tidak ditemukan atau variabel tidak ada.\")\n",
        "    embedding_model_large_vocab_global = None\n",
        "    mean_vector_for_large_vocab_global = np.zeros(300) # Fallback\n",
        "else:\n",
        "    print(f\"Mencoba memuat model embedding pra-latih besar dari {pretrained_embedding_path_global}...\")\n",
        "    try:\n",
        "        embedding_model_large_vocab_global = KeyedVectors.load_word2vec_format(pretrained_embedding_path_global, binary=False)\n",
        "        print(\"Model embedding pra-latih besar (cc.id.300.vec) berhasil dimuat.\")\n",
        "        print(f\"  Ukuran Vocabulary: {len(embedding_model_large_vocab_global.key_to_index)} kata\")\n",
        "        print(f\"  Dimensi Vektor: {embedding_model_large_vocab_global.vector_size}\")\n",
        "        if embedding_model_large_vocab_global.vectors.size > 0:\n",
        "            mean_vector_for_large_vocab_global = np.mean(embedding_model_large_vocab_global.vectors, axis=0)\n",
        "        else:\n",
        "            # Fallback jika .vectors kosong (seharusnya tidak terjadi untuk model valid)\n",
        "            mean_vector_for_large_vocab_global = np.zeros(embedding_model_large_vocab_global.vector_size)\n",
        "        print(\"  Mean vector untuk model pra-latih besar telah dihitung.\")\n",
        "\n",
        "        # Contoh evaluasi kemiripan (opsional)\n",
        "        sample_text_eval1_g2 = \"raja adalah pemimpin kerajaan\"\n",
        "        sample_text_eval2_g2 = \"ratu adalah pemimpin wanita di istana\"\n",
        "        vec_eval1_g2 = text_to_vector_for_similarity(sample_text_eval1_g2, embedding_model_large_vocab_global, mean_vector_for_large_vocab_global)\n",
        "        vec_eval2_g2 = text_to_vector_for_similarity(sample_text_eval2_g2, embedding_model_large_vocab_global, mean_vector_for_large_vocab_global)\n",
        "        similarity_score_eval_g2 = calculate_cosine_similarity(vec_eval1_g2, vec_eval2_g2)\n",
        "        print(f\"  Contoh similarity antara '{sample_text_eval1_g2}' dan '{sample_text_eval2_g2}': {similarity_score_eval_g2:.4f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal memuat model embedding pra-latih besar: {e}\")\n",
        "        embedding_model_large_vocab_global = None\n",
        "        mean_vector_for_large_vocab_global = np.zeros(300) # Fallback\n",
        "\n",
        "if embedding_model_large_vocab_global is None:\n",
        "    print(\"PERINGATAN: Model embedding pra-latih besar (cc.id.300.vec) gagal dimuat. Fungsi yang bergantung padanya mungkin tidak bekerja.\")\n",
        "print(\"Pemuatan model embedding pra-latih besar selesai.\\n\")"
      ],
      "metadata": {
        "id": "JiglJamFeGXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Cek Keberadaan Model Embedding Fine-Tuned yang Sudah Ada ===\n",
        "print(\"--- GRUP 2: SEL 5 ---\")\n",
        "skip_embedding_training = False\n",
        "# ft_model_global dan mean_fasttext_vector_global diinisialisasi di Grup 1\n",
        "\n",
        "# Path dari Grup 1\n",
        "# glove_finetuned_model_path_global\n",
        "# mean_fasttext_vector_file_path_global\n",
        "\n",
        "if os.path.exists(glove_finetuned_model_path_global) and os.path.exists(mean_fasttext_vector_file_path_global):\n",
        "    print(f\"File model fine-tuned '{os.path.basename(glove_finetuned_model_path_global)}' dan mean vector ditemukan.\")\n",
        "    try:\n",
        "        print(f\"Mencoba memuat ft_model_global dari: {glove_finetuned_model_path_global}\")\n",
        "        ft_model_global = FastText.load(glove_finetuned_model_path_global)\n",
        "        print(f\"  ft_model_global (model '{os.path.basename(glove_finetuned_model_path_global)}') berhasil dimuat.\")\n",
        "        print(f\"    Dimensi vektor: {ft_model_global.wv.vector_size}, Vocab: {len(ft_model_global.wv.key_to_index)}\")\n",
        "\n",
        "        print(f\"Mencoba memuat mean_fasttext_vector_global dari: {mean_fasttext_vector_file_path_global}\")\n",
        "        mean_fasttext_vector_global = np.load(mean_fasttext_vector_file_path_global)\n",
        "        print(f\"  mean_fasttext_vector_global berhasil dimuat dengan shape: {mean_fasttext_vector_global.shape}\")\n",
        "\n",
        "        if ft_model_global.wv.vector_size != mean_fasttext_vector_global.shape[0]:\n",
        "             print(\"PERINGATAN: Dimensi vektor ft_model_global tidak cocok dengan mean_fasttext_vector_global. Training mungkin diperlukan.\")\n",
        "             skip_embedding_training = False\n",
        "        else:\n",
        "            print(\"Model embedding fine-tuned dan mean vector berhasil dimuat. Training embedding akan dilewati.\")\n",
        "            skip_embedding_training = True\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal memuat model/mean vector yang sudah ada: {e}. Akan melanjutkan dengan training embedding.\")\n",
        "        skip_embedding_training = False\n",
        "        ft_model_global = None # Reset jika gagal load\n",
        "        mean_fasttext_vector_global = np.zeros(embedding_model_large_vocab_global.vector_size if embedding_model_large_vocab_global else 300)\n",
        "else:\n",
        "    print(f\"File model fine-tuned '{os.path.basename(glove_finetuned_model_path_global)}' atau mean vector tidak ditemukan. Akan melanjutkan dengan training embedding.\")\n",
        "    skip_embedding_training = False\n",
        "    ft_model_global = None\n",
        "    mean_fasttext_vector_global = np.zeros(embedding_model_large_vocab_global.vector_size if embedding_model_large_vocab_global else 300)\n",
        "\n",
        "print(f\"Status skip_embedding_training: {skip_embedding_training}\\n\")"
      ],
      "metadata": {
        "id": "vjASz_AReI7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Konfigurasi Parameter untuk Fine-Tuning Embedding ===\n",
        "print(\"--- GRUP 2: SEL 6 ---\")\n",
        "if not skip_embedding_training:\n",
        "    EPOCHS_EMBEDDING_G2 = 30       # Jumlah epoch untuk fine-tuning embedding (bisa disesuaikan)\n",
        "    PATIENCE_EMBEDDING_G2 = 5      # Patience untuk early stopping manual (misalnya, jika similarity tidak meningkat)\n",
        "    MIN_DELTA_EMBEDDING_G2 = 0.001 # Delta minimum untuk dianggap sebagai peningkatan similarity\n",
        "\n",
        "    # Path dari Grup 1\n",
        "    # embedding_training_log_file_global\n",
        "    # embedding_training_metadata_path_global\n",
        "    # EMBEDDING_BEST_KV_SUBDIR (digunakan oleh logger)\n",
        "    # EMBEDDING_FINAL_MODEL_SUBDIR (digunakan oleh logger)\n",
        "\n",
        "    print(f\"Parameter fine-tuning embedding diatur: Epochs={EPOCHS_EMBEDDING_G2}, Patience={PATIENCE_EMBEDDING_G2}\")\n",
        "    print(f\"Path log training embedding: {embedding_training_log_file_global}\")\n",
        "    print(f\"Path metadata training embedding: {embedding_training_metadata_path_global}\")\n",
        "else:\n",
        "    print(\"Training embedding dilewati, parameter tidak diatur.\")\n",
        "print(\"Konfigurasi parameter embedding selesai.\\n\")"
      ],
      "metadata": {
        "id": "P6E4IKZkeLKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 7: Kelas EmbeddingTrainingLogger (Callback Gensim) ===\n",
        "print(\"--- GRUP 2: SEL 7 ---\")\n",
        "\n",
        "class EmbeddingTrainingLogger(CallbackAny2Vec):\n",
        "    \"\"\"Callback untuk logging selama training FastText dan menyimpan model terbaik.\"\"\"\n",
        "    def __init__(self, best_kv_dir, final_model_dir, log_file_path_param, vector_size_param,\n",
        "                 eval_word1=\"fotosintesis\", eval_word2=\"tumbuhan\", patience=3, min_delta=0.001):\n",
        "        self.epoch_manual = 0\n",
        "        self.logs_data = {\n",
        "            \"start_time\": datetime.now().isoformat(),\n",
        "            \"parameters\": {\n",
        "                \"vector_size\": vector_size_param,\n",
        "                \"eval_word1\": eval_word1,\n",
        "                \"eval_word2\": eval_word2,\n",
        "                \"patience_manual_early_stop\": patience,\n",
        "                \"min_delta_manual_early_stop\": min_delta\n",
        "            },\n",
        "            \"early_stopping_triggered_manual\": False,\n",
        "            \"epochs_detail\": []\n",
        "        }\n",
        "        self.best_similarity_metric_val = -1.0 # Inisialisasi dengan nilai yang sangat rendah\n",
        "        self.best_model_state_for_kv_val = None # Untuk menyimpan state KeyedVectors terbaik\n",
        "        self.wait_manual_early_stop = 0\n",
        "        self.patience_manual_early_stop = patience\n",
        "        self.min_delta_manual_early_stop = min_delta\n",
        "\n",
        "        self.best_kv_dir_path = best_kv_dir\n",
        "        self.final_model_dir_path = final_model_dir\n",
        "        self.log_file_path_val = log_file_path_param\n",
        "        self.vector_size_val = vector_size_param\n",
        "        self.eval_word1_val = stemmer_global.stem(eval_word1) # Stem kata evaluasi\n",
        "        self.eval_word2_val = stemmer_global.stem(eval_word2) # Stem kata evaluasi\n",
        "\n",
        "        os.makedirs(self.best_kv_dir_path, exist_ok=True)\n",
        "        os.makedirs(self.final_model_dir_path, exist_ok=True)\n",
        "        print(f\"EmbeddingTrainingLogger diinisialisasi. Model terbaik KV akan disimpan di '{self.best_kv_dir_path}', model final di '{self.final_model_dir_path}'.\")\n",
        "        print(f\"  Evaluasi similarity pada '{self.eval_word1_val}' dan '{self.eval_word2_val}'.\")\n",
        "\n",
        "    def log_manual_epoch_end(self, model, current_manual_epoch_num, early_stopping_status_flag=False):\n",
        "        self.epoch_manual = current_manual_epoch_num\n",
        "        current_similarity = 0.0\n",
        "\n",
        "        # Pastikan kata ada di vocab sebelum menghitung similarity\n",
        "        if self.eval_word1_val in model.wv.key_to_index and self.eval_word2_val in model.wv.key_to_index:\n",
        "            current_similarity = model.wv.similarity(self.eval_word1_val, self.eval_word2_val)\n",
        "        else:\n",
        "            print(f\"  Peringatan Logger: '{self.eval_word1_val}' atau '{self.eval_word2_val}' tidak ada di vocab model. Similarity akan 0.\")\n",
        "\n",
        "        epoch_log_data = {\n",
        "            \"epoch_manual\": self.epoch_manual,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"vocab_size\": len(model.wv.key_to_index),\n",
        "            f\"similarity_sample_{self.eval_word1_val}_{self.eval_word2_val}\": float(current_similarity),\n",
        "            \"early_stopping_triggered_this_epoch\": early_stopping_status_flag\n",
        "        }\n",
        "        self.logs_data[\"epochs_detail\"].append(epoch_log_data)\n",
        "        self.logs_data[\"early_stopping_triggered_manual\"] = early_stopping_status_flag\n",
        "\n",
        "        # Simpan state KeyedVectors terbaik berdasarkan similarity\n",
        "        if float(current_similarity) > self.best_similarity_metric_val + self.min_delta_manual_early_stop :\n",
        "            self.best_similarity_metric_val = float(current_similarity)\n",
        "            # Simpan seluruh model FastText saat ini sebagai kandidat terbaik untuk KV\n",
        "            # karena KeyedVectors adalah bagian dari model FastText\n",
        "            # Kita akan menyimpannya sebagai .kv di on_train_end jika ini tetap yang terbaik\n",
        "            self.best_model_state_for_kv_val = model # Simpan referensi ke model, atau deepcopy jika khawatir termodifikasi\n",
        "            print(f\"    State KeyedVectors terbaik (dari model) diupdate pada epoch manual {self.epoch_manual} dengan similarity ({self.eval_word1_val}-{self.eval_word2_val}): {current_similarity:.4f}\")\n",
        "            self.wait_manual_early_stop = 0\n",
        "        else:\n",
        "            self.wait_manual_early_stop +=1\n",
        "            if self.wait_manual_early_stop >= self.patience_manual_early_stop:\n",
        "                 print(f\"    Patience early stopping manual ({self.patience_manual_early_stop} epoch) tercapai.\")\n",
        "                 # Flag akan di-set oleh loop training utama\n",
        "\n",
        "        try:\n",
        "            with open(self.log_file_path_val, 'w') as f: json.dump(self.logs_data, f, indent=2)\n",
        "        except Exception as e: print(f\"    Gagal menyimpan log embedding: {str(e)}\")\n",
        "\n",
        "        print(f\"  Epoch Embedding Manual {self.epoch_manual} | Similarity ({self.eval_word1_val}-{self.eval_word2_val}): {current_similarity:.4f}\")\n",
        "        if early_stopping_status_flag: print(f\"    Early stopping manual terpicu pada epoch {self.epoch_manual}.\")\n",
        "\n",
        "\n",
        "    def on_train_end(self, model):\n",
        "        \"\"\"Dipanggil di akhir semua epoch fine-tuning.\"\"\"\n",
        "        self.logs_data[\"end_time\"] = datetime.now().isoformat()\n",
        "        self.logs_data[\"total_manual_epochs_trained\"] = self.epoch_manual\n",
        "\n",
        "        # Path dari Grup 1, dengan penamaan \"glove_\"\n",
        "        # glove_final_model_internal_path_global\n",
        "        # embedding_best_kv_file_path_global\n",
        "\n",
        "        # Simpan model FastText final (state terakhir setelah semua epoch loop manual)\n",
        "        # Ini BUKAN model utama yang akan kita sebut glove_finetuned.model, tapi log dari logger\n",
        "        final_model_logger_path = os.path.join(self.final_model_dir_path, 'glove_final_model.model') # Sesuai struktur\n",
        "        try:\n",
        "            model.save(final_model_logger_path)\n",
        "            print(f\"Model FastText final (disimpan oleh logger) disimpan di: {final_model_logger_path}\")\n",
        "        except Exception as e: print(f\"Gagal menyimpan model FastText final oleh logger: {e}\")\n",
        "\n",
        "        # Simpan KeyedVectors terbaik (jika ada peningkatan)\n",
        "        if self.best_model_state_for_kv_val is not None:\n",
        "            # best_model_state_for_kv_val adalah instance model FastText yang memberikan similarity terbaik\n",
        "            # Kita simpan wv-nya sebagai KeyedVectors\n",
        "            best_kv_path = os.path.join(self.best_kv_dir_path, 'glove_best_similarity_keyedvectors.kv') # Sesuai struktur\n",
        "            try:\n",
        "                self.best_model_state_for_kv_val.wv.save(best_kv_path)\n",
        "                print(f\"KeyedVectors terbaik (berdasarkan similarity) disimpan di: {best_kv_path}\")\n",
        "            except Exception as e: print(f\"Error saat menyimpan KeyedVectors terbaik: {e}\")\n",
        "        else: print(\"Tidak ada state KeyedVectors terbaik yang disimpan (similarity tidak pernah meningkat atau model awal lebih baik).\")\n",
        "\n",
        "        try:\n",
        "            with open(self.log_file_path_val, 'w') as f: json.dump(self.logs_data, f, indent=2)\n",
        "            print(f\"Log training embedding final disimpan di {self.log_file_path_val}\")\n",
        "        except Exception as e: print(f\"Gagal menyimpan log embedding final: {str(e)}\")\n",
        "\n",
        "if not skip_embedding_training:\n",
        "    print(\"Kelas EmbeddingTrainingLogger telah didefinisikan.\")\n",
        "else:\n",
        "    print(\"Training embedding dilewati, kelas EmbeddingTrainingLogger tidak perlu didefinisikan ulang jika sudah ada.\")\n",
        "print(\"Definisi callback logger embedding selesai.\\n\")"
      ],
      "metadata": {
        "id": "WeJtdRvReOJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 8: Inisialisasi Logger untuk Fine-Tuning Embedding ===\n",
        "print(\"--- GRUP 2: SEL 8 ---\")\n",
        "embedding_ft_logger_g2 = None # Inisialisasi\n",
        "\n",
        "if not skip_embedding_training:\n",
        "    embedding_model_vector_size_g2 = 300 # Default\n",
        "    if embedding_model_large_vocab_global is not None: # Gunakan dimensi dari model pra-latih besar\n",
        "        embedding_model_vector_size_g2 = embedding_model_large_vocab_global.vector_size\n",
        "        print(f\"Menggunakan vector_size={embedding_model_vector_size_g2} dari model pra-latih besar untuk logger.\")\n",
        "    else:\n",
        "        print(f\"Peringatan: Model pra-latih besar tidak dimuat. Menggunakan vector_size default = {embedding_model_vector_size_g2} untuk logger.\")\n",
        "\n",
        "\n",
        "    # Path dari Grup 1\n",
        "    # EMBEDDING_BEST_KV_SUBDIR, EMBEDDING_FINAL_MODEL_SUBDIR, embedding_training_log_file_global\n",
        "    embedding_ft_logger_g2 = EmbeddingTrainingLogger(\n",
        "        best_kv_dir=EMBEDDING_BEST_KV_SUBDIR,\n",
        "        final_model_dir=EMBEDDING_FINAL_MODEL_SUBDIR,\n",
        "        log_file_path_param=embedding_training_log_file_global,\n",
        "        vector_size_param=embedding_model_vector_size_g2,\n",
        "        patience=PATIENCE_EMBEDDING_G2, # Menggunakan parameter dari Cell 6\n",
        "        min_delta=MIN_DELTA_EMBEDDING_G2 # Menggunakan parameter dari Cell 6\n",
        "    )\n",
        "    print(\"EmbeddingTrainingLogger diinisialisasi dan siap untuk fine-tuning.\")\n",
        "else:\n",
        "    print(\"Training embedding dilewati, logger tidak diinisialisasi.\")\n",
        "print(\"Inisialisasi logger selesai.\\n\")"
      ],
      "metadata": {
        "id": "qj6gQuIYeTsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 9: Proses Fine-Tuning Embedding (Menggunakan FastText, dinamai seolah GloVe) ===\n",
        "print(\"--- GRUP 2: SEL 9 ---\")\n",
        "# ft_model_global akan diisi di sini jika training berjalan\n",
        "\n",
        "if not skip_embedding_training:\n",
        "    if not corpus_for_embedding_fine_tuning:\n",
        "        print(\"Error: Korpus 'corpus_for_embedding_fine_tuning' kosong. Training embedding tidak dapat dilanjutkan.\")\n",
        "    elif embedding_ft_logger_g2 is None:\n",
        "        print(\"Error: Logger embedding (embedding_ft_logger_g2) tidak diinisialisasi. Training embedding tidak dapat dilanjutkan.\")\n",
        "    else:\n",
        "        print(f\"Korpus untuk training embedding ('corpus_for_embedding_fine_tuning') siap dengan {len(corpus_for_embedding_fine_tuning)} dokumen/kalimat.\")\n",
        "        if corpus_for_embedding_fine_tuning: print(f\"  Contoh item pertama di korpus: {corpus_for_embedding_fine_tuning[0][:10]}...\")\n",
        "\n",
        "        # Parameter untuk model FastText baru (training dari awal)\n",
        "        # Dimensi vektor diambil dari logger (yang mengambil dari model pra-latih besar)\n",
        "        current_ft_vector_size = embedding_ft_logger_g2.vector_size_val\n",
        "        GENSIM_INTERNAL_EPOCHS_PER_MANUAL_LOOP = 5 # Epoch internal Gensim per loop manual kita\n",
        "        FT_WINDOW_SIZE = 5\n",
        "        FT_MIN_COUNT = 2 # Abaikan kata dengan frekuensi di bawah ini\n",
        "        FT_WORKERS = 4   # Jumlah worker threads\n",
        "        FT_SG = 1        # 1 untuk skip-gram; 0 untuk CBOW\n",
        "        FT_HS = 0        # 0 untuk negative sampling; 1 untuk hierarchical softmax\n",
        "        FT_NEGATIVE = 10 # Jumlah noise words untuk negative sampling\n",
        "\n",
        "        print(f\"\\nMenginisialisasi model FastText baru (training dari awal) dengan vector_size: {current_ft_vector_size}\")\n",
        "        temp_ft_model = FastText(\n",
        "            vector_size=current_ft_vector_size,\n",
        "            window=FT_WINDOW_SIZE,\n",
        "            min_count=FT_MIN_COUNT,\n",
        "            workers=FT_WORKERS,\n",
        "            sg=FT_SG,\n",
        "            hs=FT_HS,\n",
        "            negative=FT_NEGATIVE,\n",
        "            epochs=GENSIM_INTERNAL_EPOCHS_PER_MANUAL_LOOP, # Epoch internal Gensim\n",
        "            callbacks=[embedding_ft_logger_g2] # Tambahkan callback internal Gensim jika perlu, tapi kita pakai loop manual\n",
        "        )\n",
        "\n",
        "        print(\"Membangun vocabulary model FastText dari korpus...\")\n",
        "        temp_ft_model.build_vocab(corpus_iterable=corpus_for_embedding_fine_tuning)\n",
        "        print(f\"  Ukuran vocabulary dari korpus: {len(temp_ft_model.wv.key_to_index)} kata\")\n",
        "        print(f\"  Shape temp_ft_model.wv.vectors: {temp_ft_model.wv.vectors.shape}\")\n",
        "\n",
        "        # TIDAK menggunakan intersect_word2vec_format karena kita melatih dari awal\n",
        "        # Jika ingin inisialisasi dengan pretrained besar:\n",
        "        # if embedding_model_large_vocab_global is not None:\n",
        "        #     print(\"Melakukan intersect_word2vec_format dengan model pra-latih besar...\")\n",
        "        #     temp_ft_model.intersect_word2vec_format(pretrained_embedding_path_global, binary=False, lockf=1.0)\n",
        "        # else:\n",
        "        #     print(\"Model pra-latih besar tidak tersedia untuk intersect, training dari scratch.\")\n",
        "\n",
        "        print(f\"\\nMemulai training embedding (maks {EPOCHS_EMBEDDING_G2} epoch manual, early stopping patience={PATIENCE_EMBEDDING_G2})...\")\n",
        "        actual_epochs_trained_embedding_g2 = 0\n",
        "\n",
        "        for epoch_manual_num in range(1, EPOCHS_EMBEDDING_G2 + 1):\n",
        "            actual_epochs_trained_embedding_g2 = epoch_manual_num\n",
        "            print(f\"\\n--- Epoch Manual Training Embedding ke-{epoch_manual_num}/{EPOCHS_EMBEDDING_G2} ---\")\n",
        "\n",
        "            # Latih model FastText\n",
        "            temp_ft_model.train(\n",
        "                corpus_for_embedding_fine_tuning,\n",
        "                total_examples=temp_ft_model.corpus_count,\n",
        "                epochs=temp_ft_model.epochs # Menggunakan epochs yang diset saat inisialisasi model\n",
        "            )\n",
        "\n",
        "            # Logika Early Stopping Manual (dikontrol oleh logger sekarang)\n",
        "            # Panggil log_manual_epoch_end dari logger, yang juga akan cek similarity\n",
        "            # Dapatkan status early stopping dari logger\n",
        "            embedding_ft_logger_g2.log_manual_epoch_end(temp_ft_model, epoch_manual_num,\n",
        "                                                         embedding_ft_logger_g2.wait_manual_early_stop >= embedding_ft_logger_g2.patience_manual_early_stop)\n",
        "\n",
        "            if embedding_ft_logger_g2.logs_data[\"early_stopping_triggered_manual\"]:\n",
        "                print(f\"Early stopping manual terpicu setelah epoch {epoch_manual_num}. Menghentikan training embedding.\")\n",
        "                break\n",
        "        # Akhir loop epoch manual\n",
        "\n",
        "        embedding_ft_logger_g2.on_train_end(temp_ft_model) # Panggil on_train_end dari logger\n",
        "        ft_model_global = temp_ft_model # Assign model yang sudah dilatih ke variabel global\n",
        "        print(\"\\n--- Training embedding (dari awal) selesai ---\")\n",
        "\n",
        "        # Hitung dan simpan mean_fasttext_vector_global\n",
        "        if ft_model_global is not None and hasattr(ft_model_global, 'wv') and ft_model_global.wv.vectors.size > 0:\n",
        "            mean_fasttext_vector_global = np.mean(ft_model_global.wv.vectors, axis=0)\n",
        "            try:\n",
        "                np.save(mean_fasttext_vector_file_path_global, mean_fasttext_vector_global)\n",
        "                print(f\"Mean vector dari ft_model_global berhasil disimpan di: {mean_fasttext_vector_file_path_global}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Gagal menyimpan mean_fasttext_vector_global: {e}\")\n",
        "        else:\n",
        "            print(\"ft_model_global tidak valid atau tidak memiliki vektor, mean_fasttext_vector_global tidak dihitung/disimpan.\")\n",
        "            # Fallback jika mean_fasttext_vector_global masih nol dari inisialisasi global\n",
        "            if np.all(mean_fasttext_vector_global == 0):\n",
        "                 mean_fasttext_vector_global = np.zeros(current_ft_vector_size if 'current_ft_vector_size' in locals() else 300)\n",
        "\n",
        "\n",
        "else: # skip_embedding_training is True\n",
        "    print(\"Training embedding dilewati karena model dan mean vector sudah ada dan dimuat.\")\n",
        "    # Pastikan ft_model_global dan mean_fasttext_vector_global sudah dimuat dengan benar di Cell 5\n",
        "\n",
        "# Validasi akhir untuk ft_model_global dan mean_fasttext_vector_global\n",
        "if ft_model_global is None:\n",
        "    print(\"PERINGATAN KRITIS: ft_model_global tidak terdefinisi setelah Grup 2, Sel 9.\")\n",
        "else:\n",
        "    print(f\"ft_model_global siap digunakan (Vocab: {len(ft_model_global.wv.key_to_index) if hasattr(ft_model_global, 'wv') else 'N/A'}).\")\n",
        "\n",
        "if not isinstance(mean_fasttext_vector_global, np.ndarray) or mean_fasttext_vector_global.ndim != 1:\n",
        "    print(f\"PERINGATAN KRITIS: mean_fasttext_vector_global tidak valid setelah Grup 2, Sel 9. Shape: {mean_fasttext_vector_global.shape if isinstance(mean_fasttext_vector_global, np.ndarray) else type(mean_fasttext_vector_global)}\")\n",
        "else:\n",
        "     print(f\"mean_fasttext_vector_global siap digunakan (Shape: {mean_fasttext_vector_global.shape}).\")\n",
        "\n",
        "print(\"Proses fine-tuning embedding selesai.\\n\")"
      ],
      "metadata": {
        "id": "xygoJ9YSeU5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 10: Penyimpanan Akhir Model Embedding Utama dan Metadata ===\n",
        "# (Sebelumnya Sel 11 di notebook asli Anda, disesuaikan)\n",
        "print(\"--- GRUP 2: SEL 10 ---\")\n",
        "\n",
        "if ft_model_global is not None:\n",
        "    # Simpan model FastText utama yang akan digunakan (hasil fine-tuning atau yang dimuat)\n",
        "    # Ini akan menjadi model yang kita sebut \"glove_finetuned.model\"\n",
        "    # Path dari Grup 1: glove_finetuned_model_path_global\n",
        "    try:\n",
        "        ft_model_global.save(glove_finetuned_model_path_global)\n",
        "        print(f\"Model FastText utama (glove_finetuned.model) berhasil disimpan/diperbarui di: {glove_finetuned_model_path_global}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal menyimpan model FastText utama '{glove_finetuned_model_path_global}': {e}\")\n",
        "\n",
        "    # Menyiapkan dan menyimpan metadata embedding\n",
        "    # Path dari Grup 1: embedding_training_metadata_path_global\n",
        "\n",
        "    # Dapatkan data dari logger jika training dilakukan, atau set default jika dilewati\n",
        "    total_epochs_trained_val = 0\n",
        "    early_stopping_triggered_val = False\n",
        "    final_similarity_for_metadata_display_val = 0.0\n",
        "    best_similarity_metric_for_metadata_val = 0.0\n",
        "    eval_word1_md = \"fotosintesis\" # Kata default untuk display\n",
        "    eval_word2_md = \"tumbuhan\"   # Kata default untuk display\n",
        "\n",
        "\n",
        "    if not skip_embedding_training and embedding_ft_logger_g2 is not None: # Jika training dijalankan\n",
        "        total_epochs_trained_val = embedding_ft_logger_g2.epoch_manual\n",
        "        early_stopping_triggered_val = embedding_ft_logger_g2.logs_data[\"early_stopping_triggered_manual\"]\n",
        "        eval_word1_md = embedding_ft_logger_g2.eval_word1_val\n",
        "        eval_word2_md = embedding_ft_logger_g2.eval_word2_val\n",
        "        if eval_word1_md in ft_model_global.wv.key_to_index and eval_word2_md in ft_model_global.wv.key_to_index:\n",
        "            final_similarity_for_metadata_display_val = ft_model_global.wv.similarity(eval_word1_md, eval_word2_md)\n",
        "        best_similarity_metric_for_metadata_val = embedding_ft_logger_g2.best_similarity_metric_val\n",
        "    elif ft_model_global is not None: # Jika training dilewati, coba hitung similarity dari model yang dimuat\n",
        "        eval_word1_md_stemmed = stemmer_global.stem(eval_word1_md)\n",
        "        eval_word2_md_stemmed = stemmer_global.stem(eval_word2_md)\n",
        "        if eval_word1_md_stemmed in ft_model_global.wv.key_to_index and eval_word2_md_stemmed in ft_model_global.wv.key_to_index:\n",
        "            final_similarity_for_metadata_display_val = ft_model_global.wv.similarity(eval_word1_md_stemmed, eval_word2_md_stemmed)\n",
        "        # best_similarity_metric_for_metadata_val akan 0 jika training dilewati dan tidak ada data dari log lama\n",
        "        eval_word1_md = eval_word1_md_stemmed\n",
        "        eval_word2_md = eval_word2_md_stemmed\n",
        "\n",
        "\n",
        "    # Hitung coverage pada dataset yang digunakan untuk fine-tuning\n",
        "    coverage_train_val = 0.0\n",
        "    if corpus_for_embedding_fine_tuning and ft_model_global:\n",
        "        all_words_in_corpus_set = set(word for sentence in corpus_for_embedding_fine_tuning for word in sentence)\n",
        "        words_in_vocab_and_corpus = [word for word in all_words_in_corpus_set if word in ft_model_global.wv.key_to_index]\n",
        "        if all_words_in_corpus_set:\n",
        "             coverage_train_val = len(words_in_vocab_and_corpus) / len(all_words_in_corpus_set)\n",
        "\n",
        "    embedding_metadata_content_g2 = {\n",
        "        'corpus_size_sentences_for_embedding': len(corpus_for_embedding_fine_tuning) if corpus_for_embedding_fine_tuning else 0,\n",
        "        'vocab_size_trained_model': len(ft_model_global.wv.key_to_index) if ft_model_global and hasattr(ft_model_global, 'wv') else 0,\n",
        "        'coverage_train_on_trained_model': float(f\"{coverage_train_val:.4f}\"), # Format sebagai float\n",
        "        'total_epochs_trained_embedding': total_epochs_trained_val,\n",
        "        'early_stopping_triggered_embedding_manual': early_stopping_triggered_val,\n",
        "        f'final_similarity_sample ({eval_word1_md}-{eval_word2_md})': float(f\"{final_similarity_for_metadata_display_val:.4f}\"),\n",
        "        f'best_similarity_sample ({eval_word1_md}-{eval_word2_md})': float(f\"{best_similarity_metric_for_metadata_val:.4f}\"),\n",
        "        'embedding_model_type': 'FastText (trained from scratch, named as GloVe)',\n",
        "        'embedding_vector_size': ft_model_global.vector_size if ft_model_global else (embedding_model_large_vocab_global.vector_size if embedding_model_large_vocab_global else 300),\n",
        "        'embedding_window_size': ft_model_global.window if ft_model_global and hasattr(ft_model_global, 'window') else \"N/A\",\n",
        "        'embedding_min_count': ft_model_global.min_count if ft_model_global and hasattr(ft_model_global, 'min_count') else \"N/A\",\n",
        "        'embedding_sg_algorithm': ft_model_global.sg if ft_model_global and hasattr(ft_model_global, 'sg') else \"N/A\",\n",
        "        'embedding_hs_algorithm': ft_model_global.hs if ft_model_global and hasattr(ft_model_global, 'hs') else \"N/A\",\n",
        "        'embedding_negative_sampling': ft_model_global.negative if ft_model_global and hasattr(ft_model_global, 'negative') else \"N/A\",\n",
        "        'embedding_training_date': datetime.now().isoformat()\n",
        "    }\n",
        "    try:\n",
        "        with open(embedding_training_metadata_path_global, 'w') as f:\n",
        "            json.dump(embedding_metadata_content_g2, f, indent=4)\n",
        "        print(f\"Metadata Training Embedding berhasil disimpan di: {embedding_training_metadata_path_global}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal menyimpan metadata training embedding: {e}\")\n",
        "\n",
        "    print(\"\\n--- Rangkuman File yang Disimpan/Dimuat untuk Embedding ---\")\n",
        "    print(f\"1. Model Embedding Utama (digunakan untuk LSTM & similarity): {glove_finetuned_model_path_global} {'(Dimuat)' if skip_embedding_training else '(Dilatih/Diperbarui)'}\")\n",
        "    print(f\"2. Mean Vector dari Model Embedding Utama: {mean_fasttext_vector_file_path_global} {'(Dimuat)' if skip_embedding_training else '(Dihitung/Disimpan)'}\")\n",
        "    if os.path.exists(embedding_best_kv_file_path_global):\n",
        "        print(f\"3. KeyedVectors Terbaik (disimpan oleh logger jika training): {embedding_best_kv_file_path_global}\")\n",
        "    if os.path.exists(glove_final_model_internal_path_global):\n",
        "         print(f\"4. Model FastText Final (disimpan oleh logger jika training): {glove_final_model_internal_path_global}\")\n",
        "    print(f\"5. Metadata Training Embedding: {embedding_training_metadata_path_global}\")\n",
        "    if os.path.exists(embedding_training_log_file_global):\n",
        "        print(f\"6. Log Training Embedding: {embedding_training_log_file_global}\")\n",
        "\n",
        "else: # ft_model_global is None\n",
        "    print(\"Model embedding fine-tuned (ft_model_global) tidak tersedia. Penyimpanan metadata dilewati.\")\n",
        "\n",
        "print(\"Penyimpanan akhir model embedding dan metadata selesai.\\n\")"
      ],
      "metadata": {
        "id": "Rodmcnk9ebO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LSTM**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZmPwKH_k4q9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GRUP 3: IMPLEMENTASI LSTM MANUAL (NumPy) ---\n",
        "\n",
        "# === Cell 1: Definisi Kelas LSTM Manual dan Fungsi Pendukung ===\n",
        "print(\"--- GRUP 3: SEL 1 ---\")\n",
        "\n",
        "# Pastikan variabel path global sudah ada dari Grup 1\n",
        "# lstm_manual_model_file_path_global\n",
        "# lstm_manual_metadata_file_path_global\n",
        "# dataset_path_global\n",
        "# mean_fasttext_vector_file_path_global\n",
        "# ft_model_global (diharapkan sudah dimuat atau dilatih di Grup 2)\n",
        "\n",
        "if 'ft_model_global' not in globals() or ft_model_global is None:\n",
        "    print(\"PERINGATAN: ft_model_global (model embedding fine-tuned) tidak ditemukan. LSTM mungkin tidak dapat dilatih atau dievaluasi dengan benar.\")\n",
        "    # Inisialisasi dummy ft_model jika tidak ada, agar tidak error, tapi ini akan menghasilkan vektor nol\n",
        "    if 'embedding_model_large_vocab_global' in globals() and embedding_model_large_vocab_global is not None:\n",
        "        print(\"Menggunakan embedding_model_large_vocab_global sebagai fallback untuk dimensi ft_model_global.\")\n",
        "        ft_model_global = embedding_model_large_vocab_global # Fallback sementara, idealnya ft_model_global ada\n",
        "    else:\n",
        "        # Jika semua model embedding tidak ada, buat dummy KeyedVectors\n",
        "        dummy_vectors = KeyedVectors(vector_size=300)\n",
        "        # Tambahkan satu kata dummy agar tidak error saat akses wv\n",
        "        dummy_vectors.add_vectors([\"<dummy>\"], [np.zeros(300)])\n",
        "        ft_model_global = type('obj', (object,), {'wv': dummy_vectors, 'vector_size': 300})() # Dummy model FastText\n",
        "        print(\"Membuat dummy ft_model_global karena tidak ada model embedding yang dimuat.\")\n",
        "\n",
        "\n",
        "if 'mean_fasttext_vector_global' not in globals() or not isinstance(mean_fasttext_vector_global, np.ndarray) or mean_fasttext_vector_global.shape[0] == 0 :\n",
        "    print(f\"PERINGATAN: mean_fasttext_vector_global tidak valid. Mencoba memuat dari path: {mean_fasttext_vector_file_path_global}\")\n",
        "    try:\n",
        "        mean_fasttext_vector_global = np.load(mean_fasttext_vector_file_path_global)\n",
        "        if not isinstance(mean_fasttext_vector_global, np.ndarray) or mean_fasttext_vector_global.shape[0] == 0: # Cek lagi setelah load\n",
        "            print(f\"  Gagal memuat mean_fasttext_vector_global yang valid. Menggunakan vektor nol (dimensi 300).\")\n",
        "            mean_fasttext_vector_global = np.zeros(ft_model_global.wv.vector_size if hasattr(ft_model_global, 'wv') else 300)\n",
        "        else:\n",
        "            print(f\"  mean_fasttext_vector_global berhasil dimuat dari file dengan shape {mean_fasttext_vector_global.shape}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Gagal memuat mean_fasttext_vector_global dari file: {e}. Menggunakan vektor nol (dimensi 300).\")\n",
        "        mean_fasttext_vector_global = np.zeros(ft_model_global.wv.vector_size if hasattr(ft_model_global, 'wv') else 300)\n",
        "\n",
        "\n",
        "def save_lstm_manual_metadata(path, metadata):\n",
        "    try:\n",
        "        with open(path, 'w') as f: json.dump(metadata, f, indent=4)\n",
        "        print(f\"Metadata LSTM manual disimpan di: {path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal menyimpan metadata LSTM manual: {e}\")\n",
        "\n",
        "def load_lstm_manual_metadata(path):\n",
        "    try:\n",
        "        with open(path, 'r') as f: return json.load(f)\n",
        "    except FileNotFoundError:\n",
        "        # print(f\"File metadata LSTM manual tidak ditemukan di: {path}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Gagal memuat metadata LSTM manual dari {path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def xavier_init(size_param, use_fan_in_fan_out=False):\n",
        "    # Implementasi inisialisasi Xavier yang lebih umum\n",
        "    if not isinstance(size_param, tuple) or len(size_param) == 0:\n",
        "        raise ValueError(\"Parameter 'size_param' harus berupa tuple yang tidak kosong.\")\n",
        "\n",
        "    if len(size_param) == 1: # Untuk bias atau input layer tunggal\n",
        "        fan_in = size_param[0]\n",
        "        fan_out = 1 # Asumsi\n",
        "    else: # Untuk weight matrix\n",
        "        fan_in = size_param[1]  # Jumlah neuron di layer sebelumnya\n",
        "        fan_out = size_param[0] # Jumlah neuron di layer saat ini\n",
        "\n",
        "    if fan_in == 0: return np.zeros(size_param) # Hindari pembagian dengan nol\n",
        "\n",
        "    if use_fan_in_fan_out: # Xavier/Glorot uniform dengan fan_in dan fan_out\n",
        "        limit = np.sqrt(6.0 / (fan_in + fan_out))\n",
        "        return np.random.uniform(-limit, limit, size=size_param)\n",
        "    else: # Implementasi asli Anda (mirip He/Lecun normal)\n",
        "        return np.random.randn(*size_param) * np.sqrt(1.0 / fan_in)\n",
        "\n",
        "\n",
        "class LSTMManual:\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, learning_rate=0.01):\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Inisialisasi bobot dan bias\n",
        "        # Matriks bobot untuk input dan hidden state sebelumnya\n",
        "        # Ukuran: (hidden_dim, hidden_dim + input_dim)\n",
        "        self.Wf = xavier_init((hidden_dim, hidden_dim + input_dim)) # Forget gate\n",
        "        self.Wi = xavier_init((hidden_dim, hidden_dim + input_dim)) # Input gate\n",
        "        self.Wo = xavier_init((hidden_dim, hidden_dim + input_dim)) # Output gate\n",
        "        self.Wc = xavier_init((hidden_dim, hidden_dim + input_dim)) # Cell state candidate\n",
        "\n",
        "        # Bias untuk setiap gate\n",
        "        # Ukuran: (hidden_dim, 1)\n",
        "        self.bf = np.zeros((hidden_dim, 1))\n",
        "        self.bi = np.zeros((hidden_dim, 1))\n",
        "        self.bo = np.zeros((hidden_dim, 1))\n",
        "        self.bc = np.zeros((hidden_dim, 1))\n",
        "\n",
        "        # Bobot dan bias untuk output layer\n",
        "        # Ukuran Wy: (output_dim, hidden_dim)\n",
        "        # Ukuran by: (output_dim, 1)\n",
        "        self.Wy = xavier_init((output_dim, hidden_dim))\n",
        "        self.by = np.zeros((output_dim, 1))\n",
        "\n",
        "        self.cache = {} # Untuk menyimpan nilai intermediate selama forward pass\n",
        "\n",
        "    def sigmoid(self, x_val):\n",
        "        return 1 / (1 + np.exp(-np.clip(x_val, -500, 500))) # Clip untuk stabilitas numerik\n",
        "\n",
        "    def tanh(self, x_val):\n",
        "        return np.tanh(np.clip(x_val, -500, 500)) # Clip untuk stabilitas numerik\n",
        "\n",
        "    def forward(self, x_sequence_input):\n",
        "        # x_sequence_input adalah list dari vektor input, setiap vektor mewakili satu timestep.\n",
        "        # Untuk kasus kita, x_sequence_input akan berisi SATU vektor (rata-rata embedding jawaban)\n",
        "        # sehingga T (jumlah timestep) akan menjadi 1.\n",
        "\n",
        "        T = len(x_sequence_input)\n",
        "        if T == 0:\n",
        "            # print(\"Peringatan Forward: x_sequence_input kosong.\")\n",
        "            return np.zeros((self.output_dim, 1)) # Kembalikan output default jika sekuens kosong\n",
        "\n",
        "        # Inisialisasi hidden state dan cell state awal\n",
        "        h_prev_val = np.zeros((self.hidden_dim, 1))\n",
        "        c_prev_val = np.zeros((self.hidden_dim, 1))\n",
        "\n",
        "        # Simpan nilai awal ke cache\n",
        "        self.cache['h'] = {0: h_prev_val}\n",
        "        self.cache['c'] = {0: c_prev_val}\n",
        "        self.cache['concat'] = {}\n",
        "        self.cache['f'] = {}\n",
        "        self.cache['i'] = {}\n",
        "        self.cache['o'] = {}\n",
        "        self.cache['c_tilde'] = {}\n",
        "        self.cache['x_sequence'] = x_sequence_input # Simpan sekuens input asli\n",
        "\n",
        "        for t_step in range(T):\n",
        "            # xt_val adalah vektor input pada timestep t\n",
        "            xt_val = x_sequence_input[t_step].reshape(self.input_dim, 1)\n",
        "\n",
        "            # Gabungkan hidden state sebelumnya (h_prev) dengan input saat ini (xt)\n",
        "            concat_val = np.vstack((self.cache['h'][t_step], xt_val))\n",
        "            self.cache['concat'][t_step + 1] = concat_val\n",
        "\n",
        "            # Hitung nilai gate\n",
        "            ft_val = self.sigmoid(np.dot(self.Wf, concat_val) + self.bf)  # Forget gate\n",
        "            it_val = self.sigmoid(np.dot(self.Wi, concat_val) + self.bi)  # Input gate\n",
        "            c_tilde_t_val = self.tanh(np.dot(self.Wc, concat_val) + self.bc) # Candidate cell state\n",
        "            ot_val = self.sigmoid(np.dot(self.Wo, concat_val) + self.bo)  # Output gate\n",
        "\n",
        "            # Hitung cell state baru dan hidden state baru\n",
        "            ct_val = ft_val * self.cache['c'][t_step] + it_val * c_tilde_t_val # Cell state\n",
        "            ht_val = ot_val * self.tanh(ct_val) # Hidden state\n",
        "\n",
        "            # Simpan nilai intermediate ke cache untuk backward pass\n",
        "            self.cache['h'][t_step + 1] = ht_val\n",
        "            self.cache['c'][t_step + 1] = ct_val\n",
        "            self.cache['f'][t_step + 1] = ft_val\n",
        "            self.cache['i'][t_step + 1] = it_val\n",
        "            self.cache['o'][t_step + 1] = ot_val\n",
        "            self.cache['c_tilde'][t_step + 1] = c_tilde_t_val\n",
        "\n",
        "        # Prediksi output menggunakan hidden state terakhir\n",
        "        y_pred_val = np.dot(self.Wy, self.cache['h'][T]) + self.by\n",
        "        # Karena kita memprediksi skor (nilai tunggal antara 0 dan 1), kita bisa tambahkan sigmoid di sini\n",
        "        # Jika output_dim = 1 dan ini adalah regresi ke skor 0-1, sigmoid mungkin cocok.\n",
        "        if self.output_dim == 1:\n",
        "             y_pred_val = self.sigmoid(y_pred_val) # Asumsi skor akhir adalah probabilitas/nilai 0-1\n",
        "\n",
        "        return y_pred_val\n",
        "\n",
        "    def backward(self, dy_pred_input):\n",
        "        T = len(self.cache['x_sequence'])\n",
        "        if T == 0: return # Tidak ada yang di-backward jika tidak ada forward pass\n",
        "\n",
        "        # Inisialisasi gradien\n",
        "        dWf_val, dWi_val, dWo_val, dWc_val = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wo), np.zeros_like(self.Wc)\n",
        "        dbf_val, dbi_val, dbo_val, dbc_val = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bo), np.zeros_like(self.bc)\n",
        "\n",
        "        # Gradien untuk output layer\n",
        "        dWy_val = np.dot(dy_pred_input, self.cache['h'][T].T)\n",
        "        dby_val = dy_pred_input\n",
        "\n",
        "        # Inisialisasi gradien untuk hidden state dan cell state yang akan di-propagate\n",
        "        dh_next_val = np.dot(self.Wy.T, dy_pred_input)\n",
        "        dc_next_val = np.zeros_like(dh_next_val) # Gradien cell state dari timestep selanjutnya (awal: 0)\n",
        "\n",
        "        # Loop backward melalui waktu\n",
        "        for t_step in reversed(range(1, T + 1)):\n",
        "            # Gradien untuk output gate (ot)\n",
        "            dot_val = dh_next_val * self.tanh(self.cache['c'][t_step]) * self.cache['o'][t_step] * (1 - self.cache['o'][t_step])\n",
        "            dWo_val += np.dot(dot_val, self.cache['concat'][t_step].T)\n",
        "            dbo_val += dot_val\n",
        "\n",
        "            # Gradien untuk cell state (ct)\n",
        "            dc_t_val = dh_next_val * self.cache['o'][t_step] * (1 - self.tanh(self.cache['c'][t_step])**2) + dc_next_val\n",
        "\n",
        "            # Gradien untuk candidate cell state (c_tilde_t)\n",
        "            dc_tilde_t_val = dc_t_val * self.cache['i'][t_step] * (1 - self.cache['c_tilde'][t_step]**2)\n",
        "            dWc_val += np.dot(dc_tilde_t_val, self.cache['concat'][t_step].T)\n",
        "            dbc_val += dc_tilde_t_val\n",
        "\n",
        "            # Gradien untuk input gate (it)\n",
        "            dit_val = dc_t_val * self.cache['c_tilde'][t_step] * self.cache['i'][t_step] * (1 - self.cache['i'][t_step])\n",
        "            dWi_val += np.dot(dit_val, self.cache['concat'][t_step].T)\n",
        "            dbi_val += dit_val\n",
        "\n",
        "            # Gradien untuk forget gate (ft)\n",
        "            c_prev_t_minus_1_val = self.cache['c'][t_step - 1] # c_prev pada timestep t adalah c pada t-1\n",
        "            dft_val = dc_t_val * c_prev_t_minus_1_val * self.cache['f'][t_step] * (1 - self.cache['f'][t_step])\n",
        "            dWf_val += np.dot(dft_val, self.cache['concat'][t_step].T)\n",
        "            dbf_val += dft_val\n",
        "\n",
        "            # Gradien untuk concatenated input (h_prev_val + xt_val)\n",
        "            d_concat_val = (np.dot(self.Wf.T, dft_val) +\n",
        "                            np.dot(self.Wi.T, dit_val) +\n",
        "                            np.dot(self.Wc.T, dc_tilde_t_val) +\n",
        "                            np.dot(self.Wo.T, dot_val))\n",
        "\n",
        "            # Pisahkan gradien untuk h_prev_val dan xt_val\n",
        "            dh_next_val = d_concat_val[:self.hidden_dim, :] # Gradien untuk hidden state sebelumnya\n",
        "            # dxt_val = d_concat_val[self.hidden_dim:, :] # Gradien untuk input (tidak digunakan untuk update bobot LSTM)\n",
        "\n",
        "            # Update gradien cell state untuk propagasi ke timestep sebelumnya\n",
        "            dc_next_val = dc_t_val * self.cache['f'][t_step]\n",
        "\n",
        "        # Update bobot dan bias (Vanilla Gradient Descent)\n",
        "        self.Wf -= self.learning_rate * np.clip(dWf_val, -1, 1) # Gradient clipping\n",
        "        self.Wi -= self.learning_rate * np.clip(dWi_val, -1, 1)\n",
        "        self.Wo -= self.learning_rate * np.clip(dWo_val, -1, 1)\n",
        "        self.Wc -= self.learning_rate * np.clip(dWc_val, -1, 1)\n",
        "\n",
        "        self.bf -= self.learning_rate * np.clip(dbf_val, -1, 1)\n",
        "        self.bi -= self.learning_rate * np.clip(dbi_val, -1, 1)\n",
        "        self.bo -= self.learning_rate * np.clip(dbo_val, -1, 1)\n",
        "        self.bc -= self.learning_rate * np.clip(dbc_val, -1, 1)\n",
        "\n",
        "        self.Wy -= self.learning_rate * np.clip(dWy_val, -1, 1)\n",
        "        self.by -= self.learning_rate * np.clip(dby_val, -1, 1)\n",
        "\n",
        "\n",
        "    def train(self, X_train_data, Y_train_data, epochs=10, batch_size=32): # Tambahkan batch_size\n",
        "        num_samples = X_train_data.shape[0]\n",
        "        if num_samples == 0:\n",
        "            print(\"PERINGATAN: Data training kosong. Training LSTM dihentikan.\")\n",
        "            return\n",
        "\n",
        "        # Asumsi Y_train_data adalah (num_samples, 1) atau (num_samples,)\n",
        "        if Y_train_data.ndim == 1:\n",
        "            Y_train_data = Y_train_data.reshape(-1, 1)\n",
        "\n",
        "        for epoch_num in range(epochs):\n",
        "            total_loss_epoch = 0\n",
        "            permutation_indices = np.random.permutation(num_samples)\n",
        "            X_train_shuffled_epoch = X_train_data[permutation_indices]\n",
        "            Y_train_shuffled_epoch = Y_train_data[permutation_indices]\n",
        "\n",
        "            for i in range(0, num_samples, batch_size):\n",
        "                X_batch = X_train_shuffled_epoch[i:i+batch_size]\n",
        "                Y_batch = Y_train_shuffled_epoch[i:i+batch_size]\n",
        "\n",
        "                # Akumulasi gradien untuk batch\n",
        "                dWf_batch, dWi_batch, dWo_batch, dWc_batch = np.zeros_like(self.Wf), np.zeros_like(self.Wi), np.zeros_like(self.Wo), np.zeros_like(self.Wc)\n",
        "                dbf_batch, dbi_batch, dbo_batch, dbc_batch = np.zeros_like(self.bf), np.zeros_like(self.bi), np.zeros_like(self.bo), np.zeros_like(self.bc)\n",
        "                dWy_batch, dby_batch = np.zeros_like(self.Wy), np.zeros_like(self.by)\n",
        "\n",
        "                batch_loss = 0\n",
        "                for j in range(X_batch.shape[0]):\n",
        "                    x_sample_item = X_batch[j] # Ini adalah satu vektor (misal, rata-rata embedding)\n",
        "                    y_true_sample_item = Y_batch[j].reshape(self.output_dim, 1)\n",
        "\n",
        "                    # Input ke LSTM adalah sekuens, meskipun hanya 1 timestep (vektor rata-rata)\n",
        "                    x_sequence_item = [x_sample_item]\n",
        "\n",
        "                    y_pred_item = self.forward(x_sequence_item)\n",
        "                    loss_item = 0.5 * np.sum((y_pred_item - y_true_sample_item)**2) # MSE Loss\n",
        "                    batch_loss += loss_item\n",
        "\n",
        "                    # Hitung gradien untuk sampel ini (dy_pred untuk MSE)\n",
        "                    # Jika output layer adalah sigmoid, dy_pred perlu penyesuaian (turunan sigmoid * (y_pred - y_true))\n",
        "                    # dy_pred_item = (y_pred_item - y_true_sample_item) # Gradien untuk MSE jika output linear\n",
        "                    # Jika y_pred_item adalah hasil sigmoid(z_output), maka d(Loss)/d(z_output) = (y_pred - y_true) * y_pred * (1 - y_pred)\n",
        "                    # Untuk kesederhanaan, kita asumsikan gradien yang masuk ke backward adalah d(Loss)/d(y_pred_final)\n",
        "                    dy_pred_item = (y_pred_item - y_true_sample_item) * y_pred_item * (1 - y_pred_item) # Jika output sigmoid & MSE\n",
        "\n",
        "                    # Panggil backward untuk menghitung gradien internal dan simpan ke cache sementara\n",
        "                    # Modifikasi backward agar mengembalikan gradien, bukan langsung update\n",
        "                    # Atau, backward mengakumulasi gradien untuk batch\n",
        "                    # Untuk sekarang, backward yang ada mengupdate bobot, jadi kita panggil saja\n",
        "                    # Ini akan menjadi Stochastic Gradient Descent jika batch_size=1, atau Mini-batch jika backward dimodif\n",
        "                    # Dengan backward yang ada, ini efektif SGD jika dipanggil per sampel.\n",
        "                    # Agar menjadi mini-batch, backward perlu diubah untuk mengakumulasi gradien\n",
        "\n",
        "                    # Untuk implementasi mini-batch sederhana tanpa mengubah backward drastis:\n",
        "                    # Hitung gradien per sampel dan akumulasi, lalu update setelah batch.\n",
        "                    # Ini membutuhkan backward untuk *return* gradien, bukan apply.\n",
        "                    # Untuk saat ini, kita biarkan backward mengupdate per sampel (SGD)\n",
        "                    self.backward(dy_pred_item) # Ini akan mengupdate bobot per sampel\n",
        "\n",
        "                total_loss_epoch += batch_loss / X_batch.shape[0] # Rata-rata loss per batch\n",
        "\n",
        "            avg_loss_epoch = total_loss_epoch / (num_samples / batch_size) # Rata-rata loss per epoch\n",
        "            if (epoch_num + 1) % 1 == 0: # Cetak setiap epoch\n",
        "                 print(f\"Epoch {epoch_num+1}/{epochs}, Average Loss: {avg_loss_epoch:.6f}\")\n",
        "\n",
        "\n",
        "    def save_model(self, path_param):\n",
        "        try:\n",
        "            np.savez(path_param,\n",
        "                     Wf=self.Wf, Wi=self.Wi, Wo=self.Wo, Wc=self.Wc,\n",
        "                     bf=self.bf, bi=self.bi, bo=self.bo, bc=self.bc,\n",
        "                     Wy=self.Wy, by=self.by,\n",
        "                     input_dim=self.input_dim, hidden_dim=self.hidden_dim,\n",
        "                     output_dim=self.output_dim, learning_rate=self.learning_rate)\n",
        "            print(f\"Model LSTM manual disimpan di: {path_param}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Gagal menyimpan model LSTM manual: {e}\")\n",
        "\n",
        "    def load_model(self, path_param):\n",
        "        try:\n",
        "            if not os.path.exists(path_param):\n",
        "                # print(f\"File model LSTM manual tidak ditemukan di: {path_param}\")\n",
        "                return False\n",
        "\n",
        "            lstm_data = np.load(path_param, allow_pickle=True)\n",
        "            self.Wf = lstm_data['Wf']; self.Wi = lstm_data['Wi']\n",
        "            self.Wo = lstm_data['Wo']; self.Wc = lstm_data['Wc']\n",
        "            self.bf = lstm_data['bf']; self.bi = lstm_data['bi']\n",
        "            self.bo = lstm_data['bo']; self.bc = lstm_data['bc']\n",
        "            self.Wy = lstm_data['Wy']; self.by = lstm_data['by']\n",
        "\n",
        "            self.input_dim = int(lstm_data['input_dim'].item())\n",
        "            self.hidden_dim = int(lstm_data['hidden_dim'].item())\n",
        "            self.output_dim = int(lstm_data['output_dim'].item())\n",
        "            self.learning_rate = float(lstm_data['learning_rate'].item())\n",
        "            # print(f\"Model LSTM manual berhasil dimuat dari: {path_param}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"Gagal memuat model LSTM manual dari {path_param}: {e}\")\n",
        "            return False\n",
        "\n",
        "    def predict(self, x_input_vector):\n",
        "        # x_input_vector adalah satu vektor (rata-rata embedding jawaban)\n",
        "        # LSTM forward mengharapkan list of timesteps\n",
        "        x_sequence_pred = [x_input_vector]\n",
        "        y_pred_output = self.forward(x_sequence_pred)\n",
        "        return y_pred_output # Ini akan jadi (output_dim, 1), misal (1,1)\n",
        "\n",
        "print(\"Kelas LSTMManual dan fungsi pendukung telah didefinisikan.\\n\")"
      ],
      "metadata": {
        "id": "Vf5JdhHdk8qq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Fungsi Persiapan Data untuk LSTM Manual ===\n",
        "print(\"--- GRUP 3: SEL 2 ---\")\n",
        "\n",
        "def get_average_embedding_for_text(text_to_embed, embedding_model_param, default_vector_param):\n",
        "    \"\"\"\n",
        "    Helper untuk mendapatkan vektor rata-rata dari teks menggunakan model embedding yang diberikan (bisa FastText atau KeyedVectors).\n",
        "    \"\"\"\n",
        "    if embedding_model_param is None:\n",
        "        return default_vector_param.copy()\n",
        "\n",
        "    if not isinstance(text_to_embed, str) or not text_to_embed.strip():\n",
        "        return default_vector_param.copy()\n",
        "\n",
        "    words_processed = preprocess_text(text_to_embed) # Menggunakan preprocess_text global\n",
        "    if not words_processed:\n",
        "        return default_vector_param.copy()\n",
        "\n",
        "    vectors_list = []\n",
        "    # Cek apakah model adalah model FastText penuh atau KeyedVectors\n",
        "    model_wv = None\n",
        "    if hasattr(embedding_model_param, 'wv'): # Model FastText penuh\n",
        "        model_wv = embedding_model_param.wv\n",
        "    elif hasattr(embedding_model_param, 'key_to_index'): # Model KeyedVectors\n",
        "        model_wv = embedding_model_param\n",
        "    else: # Tidak dikenali\n",
        "        return default_vector_param.copy()\n",
        "\n",
        "    for word in words_processed:\n",
        "        if word in model_wv:\n",
        "            vectors_list.append(model_wv[word])\n",
        "\n",
        "    if not vectors_list:\n",
        "        return default_vector_param.copy()\n",
        "\n",
        "    avg_vector = np.mean(vectors_list, axis=0)\n",
        "    # Pastikan dimensi output benar\n",
        "    if avg_vector.shape[0] != default_vector_param.shape[0]:\n",
        "        # print(f\"Peringatan: Dimensi avg_vector ({avg_vector.shape}) tidak cocok dengan default_vector ({default_vector_param.shape}). Menggunakan default.\")\n",
        "        return default_vector_param.copy()\n",
        "    return avg_vector\n",
        "\n",
        "\n",
        "def load_and_preprocess_data_for_lstm_manual(dataset_file_path,\n",
        "                                              embedding_model_for_input, # Ini adalah ft_model_global\n",
        "                                              mean_vector_for_input,   # Ini adalah mean_fasttext_vector_global\n",
        "                                              test_split_size=0.2,\n",
        "                                              random_state_split=42):\n",
        "    \"\"\"\n",
        "    Memuat dataset, melakukan preprocessing, mengubah teks menjadi vektor rata-rata,\n",
        "    dan membagi data untuk LSTM manual.\n",
        "    Target Y akan dihitung sebagai cosine similarity antara jawaban dan soal+kunci (menggunakan embedding_model_large_vocab_global).\n",
        "    \"\"\"\n",
        "    X_data_vectors = []\n",
        "    Y_data_scores = []\n",
        "    all_texts_for_lstm_input = [] # Untuk debugging atau analisis lebih lanjut\n",
        "\n",
        "    if not os.path.exists(dataset_file_path):\n",
        "        print(f\"Error: File dataset '{dataset_file_path}' tidak ditemukan.\")\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    # Pastikan model embedding untuk input LSTM (ft_model_global) dan mean vector-nya valid\n",
        "    if embedding_model_for_input is None or not hasattr(embedding_model_for_input, 'wv'):\n",
        "        print(\"Error: Model embedding untuk input LSTM (ft_model) tidak valid.\")\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "    if mean_vector_for_input is None or mean_vector_for_input.shape[0] != embedding_model_for_input.wv.vector_size:\n",
        "        print(f\"Error: Mean vector untuk input LSTM tidak valid atau dimensinya salah. Expected: {embedding_model_for_input.wv.vector_size}\")\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    # Model embedding besar untuk menghitung skor Y (cosine similarity soal+kunci vs jawaban)\n",
        "    # embedding_model_large_vocab_global dan mean_vector_for_large_vocab_global harus tersedia global\n",
        "    if embedding_model_large_vocab_global is None or mean_vector_for_large_vocab_global is None:\n",
        "        print(\"Error: Model embedding besar (cc.id.300.vec) atau mean vector-nya tidak tersedia global untuk menghitung Y.\")\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "\n",
        "\n",
        "    try:\n",
        "        with open(dataset_file_path, 'r', encoding='utf-8') as f:\n",
        "            dataset_list_g3 = json.load(f)\n",
        "        print(f\"Dataset berhasil dimuat dari '{dataset_file_path}' ({len(dataset_list_g3)} baris).\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saat memuat dataset: {e}\")\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    print(\"Memulai preprocessing teks dan pembuatan vektor untuk LSTM manual...\")\n",
        "    for i, item in enumerate(dataset_list_g3):\n",
        "        soal = item.get('pertanyaan', \"\")\n",
        "        jawaban_siswa = item.get('jawaban_siswa', \"\")\n",
        "        kata_kunci_list_item = item.get('kata_kunci', [])\n",
        "\n",
        "        if not (isinstance(soal, str) and soal.strip() and \\\n",
        "                isinstance(jawaban_siswa, str) and jawaban_siswa.strip()):\n",
        "            # print(f\"Data tidak lengkap pada item {i}, dilewati.\")\n",
        "            continue\n",
        "\n",
        "        # Input X: Vektor rata-rata dari jawaban siswa (menggunakan ft_model_global)\n",
        "        vektor_jawaban_siswa = get_average_embedding_for_text(jawaban_siswa,\n",
        "                                                              embedding_model_for_input,\n",
        "                                                              mean_vector_for_input)\n",
        "        X_data_vectors.append(vektor_jawaban_siswa)\n",
        "\n",
        "        # Target Y: Cosine similarity antara (vektor jawaban siswa) dan (vektor gabungan soal + kata kunci)\n",
        "        # Untuk Y, kita gunakan embedding_model_large_vocab_global agar lebih kaya\n",
        "        teks_referensi_y = soal\n",
        "        if isinstance(kata_kunci_list_item, list) and kata_kunci_list_item:\n",
        "            teks_referensi_y += \" \" + \" \".join(kata_kunci_list_item)\n",
        "\n",
        "        vektor_referensi_y = get_average_embedding_for_text(teks_referensi_y,\n",
        "                                                            embedding_model_large_vocab_global,\n",
        "                                                            mean_vector_for_large_vocab_global)\n",
        "        # Vektor jawaban siswa untuk Y juga dihitung dengan model besar\n",
        "        vektor_jawaban_siswa_y = get_average_embedding_for_text(jawaban_siswa,\n",
        "                                                                embedding_model_large_vocab_global,\n",
        "                                                                mean_vector_for_large_vocab_global)\n",
        "\n",
        "        similarity_y = calculate_cosine_similarity(vektor_jawaban_siswa_y, vektor_referensi_y)\n",
        "        Y_data_scores.append(similarity_y) # Skor similarity sebagai target\n",
        "\n",
        "        # Simpan teks gabungan untuk X jika perlu (untuk referensi LSTM manual Anda)\n",
        "        # Gabungkan pertanyaan, jawaban, kunci menjadi satu teks input per sampel untuk X\n",
        "        # (Pendekatan dari notebook asli Anda untuk input LSTM Keras)\n",
        "        # Untuk LSTM manual yang menerima satu vektor, kita sudah buat vektor_jawaban_siswa di atas\n",
        "        # Jika Anda ingin X adalah gabungan:\n",
        "        # kata_kunci_str = \" \".join(kata_kunci_list_item) if isinstance(kata_kunci_list_item, list) else str(kata_kunci_list_item)\n",
        "        # combined_text_for_X = f\"PERTANYAAN: {' '.join(preprocess_text(soal))} JAWABAN: {' '.join(preprocess_text(jawaban_siswa))} KUNCI: {' '.join(preprocess_text(kata_kunci_str))}\"\n",
        "        # X_vector = get_average_embedding_for_text(combined_text_for_X, embedding_model_for_input, mean_vector_for_input)\n",
        "        # X_data_vectors.append(X_vector)\n",
        "\n",
        "    if not X_data_vectors or not Y_data_scores:\n",
        "        print(\"Tidak ada data valid yang diproses untuk X atau Y.\")\n",
        "        return np.array([]), np.array([]), np.array([]), np.array([])\n",
        "\n",
        "    X_data_np = np.array(X_data_vectors, dtype=np.float32)\n",
        "    Y_data_np = np.array(Y_data_scores, dtype=np.float32).reshape(-1, 1) # Pastikan 2D untuk Y\n",
        "\n",
        "    print(f\"Preprocessing dan pembuatan vektor selesai. Shape X: {X_data_np.shape}, Shape Y: {Y_data_np.shape}\")\n",
        "\n",
        "    # Pembagian data (menggunakan fungsi train_test_split dari sklearn sesuai daftar Anda)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_data_np, Y_data_np, test_size=test_split_size, random_state=random_state_split, stratify=None\n",
        "    )\n",
        "    print(f\"Data berhasil di-split: Train X:{X_train.shape}, Y:{y_train.shape}; Test X:{X_test.shape}, Y:{y_test.shape}\")\n",
        "\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "print(\"Fungsi persiapan data untuk LSTM manual telah didefinisikan.\\n\")"
      ],
      "metadata": {
        "id": "j3xiVwCqlPoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Memanggil Fungsi Persiapan Data ===\n",
        "print(\"--- GRUP 3: SEL 3 ---\")\n",
        "\n",
        "# Pastikan ft_model_global dan mean_fasttext_vector_global sudah siap dari Grup 2\n",
        "# dataset_path_global dari Grup 1\n",
        "\n",
        "X_train_lstm_manual, X_test_lstm_manual, y_train_lstm_manual, y_test_lstm_manual = np.array([]), np.array([]), np.array([]), np.array([])\n",
        "\n",
        "if ft_model_global is not None and hasattr(ft_model_global, 'wv') and \\\n",
        "   mean_fasttext_vector_global is not None and mean_fasttext_vector_global.shape[0] > 0 and \\\n",
        "   embedding_model_large_vocab_global is not None and mean_vector_for_large_vocab_global is not None :\n",
        "\n",
        "    print(\"Memanggil load_and_preprocess_data_for_lstm_manual...\")\n",
        "    X_train_lstm_manual, X_test_lstm_manual, \\\n",
        "    y_train_lstm_manual, y_test_lstm_manual = load_and_preprocess_data_for_lstm_manual(\n",
        "        dataset_file_path=dataset_path_global,\n",
        "        embedding_model_for_input=ft_model_global, # Model fine-tuned untuk input X\n",
        "        mean_vector_for_input=mean_fasttext_vector_global,\n",
        "        test_split_size=0.2,\n",
        "        random_state_split=42\n",
        "    )\n",
        "\n",
        "    if X_train_lstm_manual.size > 0:\n",
        "        print(\"\\nContoh data setelah persiapan:\")\n",
        "        print(f\"  X_train_lstm_manual[0][:5]: {X_train_lstm_manual[0][:5]}\")\n",
        "        print(f\"  y_train_lstm_manual[0]: {y_train_lstm_manual[0]}\")\n",
        "    else:\n",
        "        print(\"Tidak ada data training yang dihasilkan.\")\n",
        "else:\n",
        "    print(\"PERINGATAN: Model embedding (ft_model_global atau embedding_model_large_vocab_global) atau mean vector tidak tersedia. Persiapan data LSTM manual tidak dapat dilanjutkan.\")\n",
        "\n",
        "print(\"Pemanggilan fungsi persiapan data selesai.\\n\")"
      ],
      "metadata": {
        "id": "i2WgYADIlSRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Inisialisasi atau Pemuatan Model LSTM Manual ===\n",
        "print(\"--- GRUP 3: SEL 4 ---\")\n",
        "\n",
        "# lstm_manual_model_instance_global diinisialisasi di Grup 1\n",
        "# lstm_manual_model_file_path_global dari Grup 1\n",
        "# ft_model_global dari Grup 2\n",
        "\n",
        "# Tentukan dimensi LSTM\n",
        "LSTM_INPUT_DIM_G3 = 300 # Default, akan diperbarui dari embedding model\n",
        "if ft_model_global is not None and hasattr(ft_model_global, 'wv'):\n",
        "    LSTM_INPUT_DIM_G3 = ft_model_global.wv.vector_size\n",
        "elif embedding_model_large_vocab_global is not None: # Fallback ke model besar jika ft_model tidak ada\n",
        "    LSTM_INPUT_DIM_G3 = embedding_model_large_vocab_global.vector_size\n",
        "\n",
        "LSTM_HIDDEN_DIM_G3 = 128  # Jumlah hidden unit (bisa disesuaikan)\n",
        "LSTM_OUTPUT_DIM_G3 = 1    # Output tunggal (skor)\n",
        "LSTM_LEARNING_RATE_G3 = 0.005 # Learning rate (bisa disesuaikan)\n",
        "LSTM_EPOCHS_G3 = 20       # Jumlah epoch training (bisa disesuaikan)\n",
        "LSTM_BATCH_SIZE_G3 = 16   # Batch size untuk training\n",
        "\n",
        "lstm_manual_model_instance_global = LSTMManual(\n",
        "    input_dim=LSTM_INPUT_DIM_G3,\n",
        "    hidden_dim=LSTM_HIDDEN_DIM_G3,\n",
        "    output_dim=LSTM_OUTPUT_DIM_G3,\n",
        "    learning_rate=LSTM_LEARNING_RATE_G3\n",
        ")\n",
        "print(f\"Instance LSTMManual dibuat dengan input_dim={LSTM_INPUT_DIM_G3}, hidden_dim={LSTM_HIDDEN_DIM_G3}.\")\n",
        "\n",
        "lstm_model_loaded_successfully_g3 = False\n",
        "if lstm_manual_model_instance_global.load_model(lstm_manual_model_file_path_global):\n",
        "    # Cek apakah dimensi model yang dimuat sesuai\n",
        "    metadata_lstm_loaded = load_lstm_manual_metadata(lstm_manual_metadata_file_path_global)\n",
        "    if metadata_lstm_loaded and \\\n",
        "       metadata_lstm_loaded.get('input_dim') == LSTM_INPUT_DIM_G3 and \\\n",
        "       metadata_lstm_loaded.get('hidden_dim') == LSTM_HIDDEN_DIM_G3 and \\\n",
        "       metadata_lstm_loaded.get('output_dim') == LSTM_OUTPUT_DIM_G3:\n",
        "        print(f\"Model LSTM manual ditemukan dan berhasil dimuat dari: {lstm_manual_model_file_path_global}\")\n",
        "        print(\"  Parameter model yang dimuat cocok dengan konfigurasi saat ini.\")\n",
        "        lstm_model_loaded_successfully_g3 = True\n",
        "    else:\n",
        "        if metadata_lstm_loaded:\n",
        "            print(f\"Model LSTM manual dimuat, TAPI parameter tidak cocok. Konfigurasi saat ini: In={LSTM_INPUT_DIM_G3}, Hid={LSTM_HIDDEN_DIM_G3}. Model: In={metadata_lstm_loaded.get('input_dim')}, Hid={metadata_lstm_loaded.get('hidden_dim')}.\")\n",
        "        else:\n",
        "            print(\"Model LSTM manual dimuat, tapi metadata tidak ditemukan atau tidak valid.\")\n",
        "        print(\"  Akan melatih ulang model LSTM.\")\n",
        "        # Reset bobot jika parameter tidak cocok\n",
        "        lstm_manual_model_instance_global = LSTMManual(\n",
        "            input_dim=LSTM_INPUT_DIM_G3, hidden_dim=LSTM_HIDDEN_DIM_G3, output_dim=LSTM_OUTPUT_DIM_G3, learning_rate=LSTM_LEARNING_RATE_G3\n",
        "        )\n",
        "else:\n",
        "    print(f\"File model LSTM manual tidak ditemukan di {lstm_manual_model_file_path_global} atau gagal dimuat.\")\n",
        "\n",
        "print(f\"Status pemuatan model LSTM manual berhasil: {lstm_model_loaded_successfully_g3}\\n\")"
      ],
      "metadata": {
        "id": "v6PFUI9dlUfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 5: Training dan Penyimpanan Model LSTM Manual ===\n",
        "print(\"--- GRUP 3: SEL 5 ---\")\n",
        "\n",
        "if not lstm_model_loaded_successfully_g3:\n",
        "    if X_train_lstm_manual.size > 0 and y_train_lstm_manual.size > 0:\n",
        "        print(f\"Memulai training LSTM manual dengan {LSTM_EPOCHS_G3} epoch, batch size {LSTM_BATCH_SIZE_G3}...\")\n",
        "        print(f\"  Input X_train shape: {X_train_lstm_manual.shape}, Y_train shape: {y_train_lstm_manual.shape}\")\n",
        "\n",
        "        lstm_manual_model_instance_global.train(\n",
        "            X_train_lstm_manual,\n",
        "            y_train_lstm_manual,\n",
        "            epochs=LSTM_EPOCHS_G3,\n",
        "            batch_size=LSTM_BATCH_SIZE_G3\n",
        "        )\n",
        "        print(\"Training LSTM manual selesai.\")\n",
        "\n",
        "        lstm_manual_model_instance_global.save_model(lstm_manual_model_file_path_global)\n",
        "\n",
        "        metadata_lstm_to_save = {\n",
        "            'train_size_samples': X_train_lstm_manual.shape[0],\n",
        "            'input_dim': lstm_manual_model_instance_global.input_dim,\n",
        "            'hidden_dim': lstm_manual_model_instance_global.hidden_dim,\n",
        "            'output_dim': lstm_manual_model_instance_global.output_dim,\n",
        "            'learning_rate': lstm_manual_model_instance_global.learning_rate,\n",
        "            'epochs_trained': LSTM_EPOCHS_G3,\n",
        "            'batch_size_trained': LSTM_BATCH_SIZE_G3,\n",
        "            'model_type': 'LSTM_Manual_NumPy',\n",
        "            'training_date': datetime.now().isoformat()\n",
        "        }\n",
        "        save_lstm_manual_metadata(lstm_manual_metadata_file_path_global, metadata_lstm_to_save)\n",
        "        print(\"Model LSTM manual dan metadata baru telah disimpan.\")\n",
        "    else:\n",
        "        print(\"PERINGATAN: Data training (X_train_lstm_manual atau y_train_lstm_manual) kosong. Training LSTM manual dilewati.\")\n",
        "else:\n",
        "    print(\"Training LSTM manual dilewati karena model sudah dimuat.\")\n",
        "\n",
        "print(\"Proses training/penyimpanan model LSTM manual selesai.\\n\")"
      ],
      "metadata": {
        "id": "pM4KoR2vlX8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 6: Fungsi Evaluasi LSTM Manual ===\n",
        "print(\"--- GRUP 3: SEL 6 ---\")\n",
        "\n",
        "def evaluate_lstm_manual(model_lstm, X_test_data, y_test_data):\n",
        "    \"\"\"Mengevaluasi model LSTM manual pada data test.\"\"\"\n",
        "    if model_lstm is None:\n",
        "        print(\"Model LSTM manual tidak tersedia untuk evaluasi.\")\n",
        "        return None\n",
        "\n",
        "    if X_test_data.size == 0 or y_test_data.size == 0:\n",
        "        print(\"Data test (X_test atau y_test) kosong. Evaluasi LSTM manual tidak dapat dilakukan.\")\n",
        "        return None\n",
        "\n",
        "    y_preds_list = []\n",
        "    for i in range(X_test_data.shape[0]):\n",
        "        x_sample_eval = X_test_data[i]\n",
        "        # LSTM forward mengharapkan list of timesteps\n",
        "        y_pred_eval = model_lstm.predict(x_sample_eval) # predict sudah menghandle [x_sample_eval]\n",
        "        y_preds_list.append(y_pred_eval.item()) # Ambil nilai skalar\n",
        "\n",
        "    y_preds_np = np.array(y_preds_list)\n",
        "    y_true_np = y_test_data.flatten() # Pastikan y_true juga 1D\n",
        "\n",
        "    if y_preds_np.shape != y_true_np.shape:\n",
        "        print(f\"PERINGATAN Evaluasi: Shape y_preds ({y_preds_np.shape}) dan y_true ({y_true_np.shape}) tidak cocok.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    mse = mean_squared_error(y_true_np, y_preds_np)\n",
        "    mae = mean_absolute_error(y_true_np, y_preds_np)\n",
        "    r2 = r2_score(y_true_np, y_preds_np)\n",
        "    rmse = sqrt(mse)\n",
        "\n",
        "    # Pearson dan Spearman\n",
        "    pearson_corr, _ = pearsonr(y_true_np, y_preds_np) if len(y_true_np) > 1 else (0,0)\n",
        "    spearman_corr, _ = spearmanr(y_true_np, y_preds_np) if len(y_true_np) > 1 else (0,0)\n",
        "\n",
        "\n",
        "    print(\"\\n--- Hasil Evaluasi Model LSTM Manual pada Data Test ---\")\n",
        "    print(f\"  Mean Squared Error (MSE)    : {mse:.4f}\")\n",
        "    print(f\"  Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "    print(f\"  Mean Absolute Error (MAE)   : {mae:.4f}\")\n",
        "    print(f\"  R^2 Score                   : {r2:.4f}\")\n",
        "    print(f\"  Pearson Correlation         : {pearson_corr:.4f}\")\n",
        "    print(f\"  Spearman Correlation        : {spearman_corr:.4f}\")\n",
        "\n",
        "\n",
        "    # Contoh Prediksi\n",
        "    num_samples_to_show_eval = min(5, X_test_data.shape[0])\n",
        "    if num_samples_to_show_eval > 0:\n",
        "        print(\"\\n  Contoh Prediksi vs Aktual:\")\n",
        "        print(f\"  {'Index':<7} | {'Aktual':<10} | {'Prediksi':<10}\")\n",
        "        print(\"  \" + \"-\" * 30)\n",
        "        sample_indices_eval = np.random.choice(X_test_data.shape[0], num_samples_to_show_eval, replace=False)\n",
        "        for idx_sample in sample_indices_eval:\n",
        "            actual_val = y_true_np[idx_sample]\n",
        "            predicted_val = y_preds_np[idx_sample]\n",
        "            print(f\"  {idx_sample:<7} | {actual_val:<10.4f} | {predicted_val:<10.4f}\")\n",
        "\n",
        "    evaluation_metrics_dict = {\n",
        "        'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2,\n",
        "        'pearson': pearson_corr, 'spearman': spearman_corr\n",
        "    }\n",
        "    return evaluation_metrics_dict\n",
        "\n",
        "# Panggil evaluasi jika model dan data test ada\n",
        "if lstm_manual_model_instance_global is not None and \\\n",
        "   X_test_lstm_manual.size > 0 and y_test_lstm_manual.size > 0:\n",
        "    print(\"\\nMengevaluasi model LSTM manual pada data test...\")\n",
        "    lstm_eval_results = evaluate_lstm_manual(lstm_manual_model_instance_global, X_test_lstm_manual, y_test_lstm_manual)\n",
        "    if lstm_eval_results:\n",
        "        # Simpan hasil evaluasi ke log jika perlu\n",
        "        try:\n",
        "            # Path dari Grup 1: lstm_training_log_file_global\n",
        "            # Kita bisa tambahkan hasil evaluasi ini ke log yang ada atau buat file baru\n",
        "            log_file_eval_path = os.path.join(LOGS_DIR, 'lstm_manual_evaluation_log.json')\n",
        "            with open(log_file_eval_path, 'w') as f_log_eval:\n",
        "                json.dump({\n",
        "                    \"evaluation_time\": datetime.now().isoformat(),\n",
        "                    \"metrics\": lstm_eval_results,\n",
        "                    \"model_file\": lstm_manual_model_file_path_global\n",
        "                }, f_log_eval, indent=4)\n",
        "            print(f\"Hasil evaluasi LSTM manual disimpan di: {log_file_eval_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Gagal menyimpan log evaluasi LSTM manual: {e}\")\n",
        "else:\n",
        "    print(\"Model LSTM manual atau data test tidak tersedia untuk evaluasi.\")\n",
        "\n",
        "print(\"Fungsi evaluasi LSTM manual selesai.\\n\")"
      ],
      "metadata": {
        "id": "RPUpKeI1lany"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "w-z6ks2Blbj6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IMPLEMENTASI ALGORITMA PENILAIAN**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mWqt9Vyamb-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GRUP 4: IMPLEMENTASI ALGORITMA PENILAIAN ---\n",
        "\n",
        "# === Cell 1: Pengecekan Variabel Global yang Dibutuhkan ===\n",
        "print(\"--- GRUP 4: SEL 1 ---\")\n",
        "\n",
        "# Variabel-variabel ini seharusnya sudah ada dari Grup 1, 2, dan 3:\n",
        "# - stemmer_global, active_stopwords_list_global (dari Grup 1, Sel 6)\n",
        "# - ft_model_global (dari Grup 2, Sel 9 atau Sel 5 jika dimuat)\n",
        "# - mean_fasttext_vector_global (dari Grup 2, Sel 9 atau Sel 5 jika dimuat)\n",
        "# - embedding_model_large_vocab_global (dari Grup 2, Sel 4)\n",
        "# - mean_vector_for_large_vocab_global (dari Grup 2, Sel 4)\n",
        "# - fungsi preprocess_text (dari Grup 2, Sel 1)\n",
        "# - fungsi calculate_cosine_similarity (dari Grup 2, Sel 1)\n",
        "\n",
        "# Pengecekan sederhana\n",
        "if 'preprocess_text' not in globals():\n",
        "    print(\"PERINGATAN: Fungsi 'preprocess_text' tidak terdefinisi secara global.\")\n",
        "if 'calculate_cosine_similarity' not in globals():\n",
        "    print(\"PERINGATAN: Fungsi 'calculate_cosine_similarity' tidak terdefinisi secara global.\")\n",
        "if 'ft_model_global' not in globals() or ft_model_global is None:\n",
        "    print(\"PERINGATAN: 'ft_model_global' (model embedding fine-tuned) tidak tersedia.\")\n",
        "if 'embedding_model_large_vocab_global' not in globals() or embedding_model_large_vocab_global is None:\n",
        "    print(\"PERINGATAN: 'embedding_model_large_vocab_global' (cc.id.300.vec) tidak tersedia.\")\n",
        "\n",
        "print(\"Pengecekan variabel global untuk Grup 4 selesai.\\n\")"
      ],
      "metadata": {
        "id": "vyFeqbNSmoCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Implementasi ROUGE Score ===\n",
        "print(\"--- GRUP 4: SEL 2 ---\")\n",
        "\n",
        "def rouge_score_g4(reference_text_param, student_text_param):\n",
        "    \"\"\"\n",
        "    Menghitung ROUGE-1 F1-score antara teks referensi dan teks siswa.\n",
        "    Menggunakan preprocess_text global.\n",
        "    \"\"\"\n",
        "    # Pastikan input adalah string dan lakukan preprocessing\n",
        "    ref_words_g4 = preprocess_text(str(reference_text_param))\n",
        "    stu_words_g4 = preprocess_text(str(student_text_param))\n",
        "\n",
        "    if not ref_words_g4 or not stu_words_g4:\n",
        "        return 0.0\n",
        "\n",
        "    common_unigrams_g4 = set(ref_words_g4) & set(stu_words_g4)\n",
        "\n",
        "    # Hindari ZeroDivisionError jika salah satu list kata kosong (meskipun sudah dicek di atas)\n",
        "    len_ref_words = len(ref_words_g4)\n",
        "    len_stu_words = len(stu_words_g4)\n",
        "\n",
        "    if len_ref_words == 0 or len_stu_words == 0: # Seharusnya tidak terjadi jika cek awal lolos\n",
        "        return 0.0\n",
        "\n",
        "    recall_g4 = len(common_unigrams_g4) / len_ref_words\n",
        "    precision_g4 = len(common_unigrams_g4) / len_stu_words\n",
        "\n",
        "    if (recall_g4 + precision_g4) == 0:\n",
        "        return 0.0\n",
        "    f1_score_rouge_g4 = 2 * (recall_g4 * precision_g4) / (recall_g4 + precision_g4)\n",
        "\n",
        "    return f1_score_rouge_g4\n",
        "\n",
        "print(\"Fungsi rouge_score_g4 telah didefinisikan.\\n\")"
      ],
      "metadata": {
        "id": "L8KWd4v8mpq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Implementasi TF-IDF Similarity ===\n",
        "print(\"--- GRUP 4: SEL 3 ---\")\n",
        "\n",
        "def tfidf_similarity_g4(reference_text_param, student_text_param, context_docs_param=None):\n",
        "    \"\"\"\n",
        "    Menghitung kemiripan kosinus TF-IDF.\n",
        "    Menggunakan preprocess_text global dan calculate_cosine_similarity global.\n",
        "    \"\"\"\n",
        "    ref_words_g4 = preprocess_text(str(reference_text_param))\n",
        "    stu_words_g4 = preprocess_text(str(student_text_param))\n",
        "\n",
        "    if not ref_words_g4 or not stu_words_g4:\n",
        "        return 0.0\n",
        "\n",
        "    vocabulary_g4 = set(ref_words_g4) | set(stu_words_g4)\n",
        "    if not vocabulary_g4: # Jika vocabulary kosong (misal semua kata adalah stopwords)\n",
        "        return 0.0\n",
        "\n",
        "\n",
        "    tf_ref_g4 = defaultdict(int)\n",
        "    for word in ref_words_g4: tf_ref_g4[word] += 1\n",
        "\n",
        "    tf_stu_g4 = defaultdict(int)\n",
        "    for word in stu_words_g4: tf_stu_g4[word] += 1\n",
        "\n",
        "    idf_g4 = defaultdict(float)\n",
        "    num_docs_in_context_g4 = 0\n",
        "    processed_context_docs_g4 = []\n",
        "\n",
        "    if context_docs_param and isinstance(context_docs_param, list):\n",
        "        num_docs_in_context_g4 = len(context_docs_param)\n",
        "        processed_context_docs_g4 = [set(preprocess_text(str(doc))) for doc in context_docs_param]\n",
        "\n",
        "    if num_docs_in_context_g4 == 0:\n",
        "        # print(\"Peringatan TF-IDF: Tidak ada 'context_docs_param' yang valid. IDF akan default.\")\n",
        "        for word in vocabulary_g4: idf_g4[word] = 1.0 # Default IDF\n",
        "    else:\n",
        "        for word in vocabulary_g4:\n",
        "            docs_containing_word_g4 = sum(1 for processed_doc_set in processed_context_docs_g4 if word in processed_doc_set)\n",
        "            idf_g4[word] = math.log((num_docs_in_context_g4 + 1) / (docs_containing_word_g4 + 1.0)) + 1.0 # Smoothing, tambah 1 agar non-negatif\n",
        "\n",
        "    sorted_vocabulary_g4 = sorted(list(vocabulary_g4))\n",
        "\n",
        "    # Normalisasi TF sebelum dikalikan dengan IDF\n",
        "    len_ref = len(ref_words_g4)\n",
        "    len_stu = len(stu_words_g4)\n",
        "\n",
        "    tfidf_vector_ref_g4 = np.array([(tf_ref_g4[word] / len_ref if len_ref > 0 else 0) * idf_g4[word] for word in sorted_vocabulary_g4])\n",
        "    tfidf_vector_stu_g4 = np.array([(tf_stu_g4[word] / len_stu if len_stu > 0 else 0) * idf_g4[word] for word in sorted_vocabulary_g4])\n",
        "\n",
        "    return calculate_cosine_similarity(tfidf_vector_ref_g4, tfidf_vector_stu_g4) # Menggunakan fungsi dari Grup 2\n",
        "\n",
        "print(\"Fungsi tfidf_similarity_g4 telah didefinisikan.\\n\")"
      ],
      "metadata": {
        "id": "aVD0mTlTm1da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Fungsi Konversi Teks ke Vektor Embedding (untuk Cosine Similarity) ===\n",
        "# Menggunakan kembali fungsi text_to_vector_for_similarity dari Grup 2, Sel 1\n",
        "# atau definisikan ulang di sini jika ada penyesuaian spesifik untuk Grup 4.\n",
        "# Untuk konsistensi, lebih baik menggunakan yang sudah ada jika fungsinya sama.\n",
        "# Kita asumsikan text_to_vector_for_similarity dari Grup 2, Sel 1 sudah cukup.\n",
        "print(\"--- GRUP 4: SEL 4 ---\")\n",
        "\n",
        "# Fungsi text_to_vector_for_cosine_eval akan menjadi alias atau pemanggilan\n",
        "# ke text_to_vector_for_similarity yang menggunakan model embedding besar (cc.id.300.vec)\n",
        "# Fungsi ini akan digunakan dalam evaluate_answer di Grup 5.\n",
        "\n",
        "def text_to_vector_for_g4_eval(text_input_param,\n",
        "                               embedding_model_param, # Akan diisi dengan embedding_model_large_vocab_global\n",
        "                               default_vector_param): # Akan diisi dengan mean_vector_for_large_vocab_global\n",
        "    \"\"\"\n",
        "    Wrapper/alias untuk text_to_vector_for_similarity.\n",
        "    Tujuan utamanya adalah untuk memastikan model dan default vector yang benar digunakan\n",
        "    saat menghitung cosine similarity di fungsi penilaian akhir.\n",
        "    \"\"\"\n",
        "    return text_to_vector_for_similarity(text_input_param, embedding_model_param, default_vector_param)\n",
        "\n",
        "print(\"Fungsi text_to_vector_for_g4_eval (menggunakan text_to_vector_for_similarity) siap.\\n\")\n",
        "\n",
        "# Fungsi untuk mendapatkan vektor input LSTM (menggunakan ft_model_global)\n",
        "# Ini mirip dengan get_average_embedding_for_text dari Grup 3, Sel 2\n",
        "def get_lstm_input_vector_g4(text_input_param):\n",
        "    \"\"\"\n",
        "    Menghasilkan vektor rata-rata dari teks untuk input LSTM manual.\n",
        "    Menggunakan ft_model_global dan mean_fasttext_vector_global.\n",
        "    \"\"\"\n",
        "    # Pastikan model dan mean vector global tersedia\n",
        "    if 'ft_model_global' not in globals() or ft_model_global is None or \\\n",
        "       'mean_fasttext_vector_global' not in globals() or mean_fasttext_vector_global is None:\n",
        "        print(\"PERINGATAN (get_lstm_input_vector_g4): ft_model_global atau mean_fasttext_vector_global tidak tersedia.\")\n",
        "        # Tentukan dimensi fallback\n",
        "        fallback_dim = 300\n",
        "        if 'ft_model_global' in globals() and ft_model_global is not None and hasattr(ft_model_global, 'wv'):\n",
        "            fallback_dim = ft_model_global.wv.vector_size\n",
        "        elif 'embedding_model_large_vocab_global' in globals() and embedding_model_large_vocab_global is not None:\n",
        "             fallback_dim = embedding_model_large_vocab_global.vector_size\n",
        "        return np.zeros(fallback_dim)\n",
        "\n",
        "\n",
        "    return get_average_embedding_for_text(text_input_param,\n",
        "                                          ft_model_global, # Model fine-tuned untuk input LSTM\n",
        "                                          mean_fasttext_vector_global)\n",
        "\n",
        "\n",
        "print(\"Fungsi get_lstm_input_vector_g4 (untuk input LSTM manual) telah didefinisikan.\\n\")"
      ],
      "metadata": {
        "id": "-mO6U4nhm3IW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INPUT DATA DAN EVALUASI**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DhccEpVUo4Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- GRUP 5: INPUT DATA DAN EVALUASI ---\n",
        "\n",
        "# === Cell 1: Persiapan Komponen Evaluasi Akhir ===\n",
        "print(\"--- GRUP 5: SEL 1 ---\")\n",
        "\n",
        "# Variabel-variabel ini seharusnya sudah siap dari grup-grup sebelumnya:\n",
        "# - embedding_model_large_vocab_global (cc.id.300.vec dari Grup 2, Sel 4)\n",
        "# - mean_vector_for_large_vocab_global (dari Grup 2, Sel 4)\n",
        "# - ft_model_global (model \"glove_finetuned\" dari Grup 2, Sel 9 atau Sel 5)\n",
        "# - mean_fasttext_vector_global (dari Grup 2, Sel 9 atau Sel 5)\n",
        "# - lstm_manual_model_instance_global (dari Grup 3, Sel 4 atau Sel 5)\n",
        "# - Fungsi-fungsi dari Grup 2 dan Grup 4 (preprocess_text, calculate_cosine_similarity,\n",
        "#   text_to_vector_for_g4_eval, get_lstm_input_vector_g4, rouge_score_g4, tfidf_similarity_g4)\n",
        "\n",
        "# Pengecekan komponen krusial untuk evaluasi\n",
        "komponen_evaluasi_siap = True\n",
        "if 'embedding_model_large_vocab_global' not in globals() or embedding_model_large_vocab_global is None:\n",
        "    print(\"PERINGATAN: 'embedding_model_large_vocab_global' tidak tersedia.\")\n",
        "    komponen_evaluasi_siap = False\n",
        "if 'mean_vector_for_large_vocab_global' not in globals() or mean_vector_for_large_vocab_global is None:\n",
        "    print(\"PERINGATAN: 'mean_vector_for_large_vocab_global' tidak tersedia.\")\n",
        "    komponen_evaluasi_siap = False\n",
        "if 'ft_model_global' not in globals() or ft_model_global is None:\n",
        "    print(\"PERINGATAN: 'ft_model_global' (untuk input LSTM & beberapa similarity) tidak tersedia.\")\n",
        "    komponen_evaluasi_siap = False\n",
        "if 'mean_fasttext_vector_global' not in globals() or mean_fasttext_vector_global is None:\n",
        "    print(\"PERINGATAN: 'mean_fasttext_vector_global' tidak tersedia.\")\n",
        "    komponen_evaluasi_siap = False\n",
        "if 'lstm_manual_model_instance_global' not in globals() or lstm_manual_model_instance_global is None:\n",
        "    print(\"PERINGATAN: 'lstm_manual_model_instance_global' tidak tersedia.\")\n",
        "    komponen_evaluasi_siap = False\n",
        "\n",
        "if komponen_evaluasi_siap:\n",
        "    print(\"Semua komponen model dan fungsi pendukung untuk evaluasi akhir tampaknya siap.\")\n",
        "else:\n",
        "    print(\"SATU ATAU LEBIH KOMPONEN KRUSIAL TIDAK SIAP. EVALUASI MUNGKIN GAGAL ATAU TIDAK AKURAT.\")\n",
        "\n",
        "print(\"Persiapan komponen evaluasi akhir selesai.\\n\")"
      ],
      "metadata": {
        "id": "M2ZzwQaro_Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 2: Definisi Fungsi evaluate_answer (dengan LSTM Manual dan Dua Threshold Scaling) ===\n",
        "print(\"--- GRUP 5: SEL 2 ---\")\n",
        "\n",
        "def evaluate_answer_g5(jawaban_siswa_text_param, soal_text_param, kata_kunci_list_param,\n",
        "                       embedding_model_for_cosine_param,   # Akan diisi dengan embedding_model_large_vocab_global\n",
        "                       mean_vector_for_cosine_param,     # Akan diisi dengan mean_vector_for_large_vocab_global\n",
        "                       embedding_model_for_lstm_input,   # Akan diisi dengan ft_model_global\n",
        "                       mean_vector_for_lstm_input,       # Akan diisi dengan mean_fasttext_vector_global\n",
        "                       lstm_model_param,                 # Akan diisi dengan lstm_manual_model_instance_global\n",
        "                       konteks_tfidf_dokumen=None # Opsional: list string dokumen untuk IDF yang lebih baik\n",
        "                       ):\n",
        "    \"\"\"\n",
        "    Mengevaluasi jawaban siswa dengan bobot dinamis dan dua kemungkinan threshold scaling,\n",
        "    menggunakan LSTM manual dan model embedding yang ditentukan.\n",
        "    \"\"\"\n",
        "    if not all([embedding_model_for_cosine_param, lstm_model_param,\n",
        "                mean_vector_for_cosine_param is not None,\n",
        "                embedding_model_for_lstm_input is not None, # Cek model untuk input LSTM\n",
        "                mean_vector_for_lstm_input is not None]): # Cek mean vector untuk input LSTM\n",
        "        print(\"  Peringatan di evaluate_answer_g5: Model/mean_vector penting tidak valid. Mengembalikan skor 0.\")\n",
        "        return 0.0\n",
        "\n",
        "    # Teks referensi untuk ROUGE dan TF-IDF adalah gabungan soal dan kata kunci\n",
        "    # Namun, untuk cosine similarity, kita akan bandingkan jawaban dengan soal saja (atau bisa disesuaikan)\n",
        "    # Sesuai notebook awal, teks referensi untuk ROUGE & TF-IDF adalah soal.\n",
        "    reference_text_for_metrics_g5 = str(soal_text_param)\n",
        "    jawaban_siswa_str = str(jawaban_siswa_text_param)\n",
        "\n",
        "    # 1. Hitung Skor ROUGE (menggunakan fungsi dari Grup 4)\n",
        "    skor_rouge_val = rouge_score_g4(reference_text_for_metrics_g5, jawaban_siswa_str)\n",
        "\n",
        "    # 2. Hitung Skor TF-IDF (menggunakan fungsi dari Grup 4)\n",
        "    # Konteks untuk TF-IDF bisa dari soal dan kata kunci\n",
        "    if konteks_tfidf_dokumen is None: # Default konteks jika tidak diberikan\n",
        "        konteks_tfidf_dokumen_internal = [reference_text_for_metrics_g5]\n",
        "        if isinstance(kata_kunci_list_param, list):\n",
        "            konteks_tfidf_dokumen_internal.extend([str(kw) for kw in kata_kunci_list_param if isinstance(kw, str) and kw.strip()])\n",
        "    else:\n",
        "        konteks_tfidf_dokumen_internal = konteks_tfidf_dokumen # Gunakan konteks yang diberikan jika ada\n",
        "\n",
        "    skor_tfidf_val = tfidf_similarity_g4(reference_text_for_metrics_g5, jawaban_siswa_str,\n",
        "                                         context_docs_param=konteks_tfidf_dokumen_internal)\n",
        "\n",
        "    # 3. Hitung Skor Cosine Embedding (menggunakan embedding_model_large_vocab_global)\n",
        "    # Vektor jawaban dan vektor soal (sebagai referensi) dihitung dengan model embedding besar\n",
        "    vector_jawaban_cosine = text_to_vector_for_g4_eval(jawaban_siswa_str,\n",
        "                                                       embedding_model_for_cosine_param,\n",
        "                                                       mean_vector_for_cosine_param)\n",
        "    vector_ref_cosine = text_to_vector_for_g4_eval(reference_text_for_metrics_g5, # Referensi cosine adalah soal\n",
        "                                                   embedding_model_for_cosine_param,\n",
        "                                                   mean_vector_for_cosine_param)\n",
        "    skor_cosine_embedding_val = calculate_cosine_similarity(vector_jawaban_cosine, vector_ref_cosine)\n",
        "\n",
        "\n",
        "    # 4. Dapatkan Skor dari Model LSTM Manual\n",
        "    # Input untuk LSTM manual adalah vektor rata-rata dari jawaban siswa (menggunakan ft_model_global)\n",
        "    vektor_jawaban_for_lstm = get_lstm_input_vector_g4(jawaban_siswa_str) # Fungsi dari Grup 4, Sel 4\n",
        "    skor_lstm_pred_val = 0.0\n",
        "    if np.all(vektor_jawaban_for_lstm == 0): # Jika vektor jawaban nol (misal, semua kata OOV atau jawaban kosong)\n",
        "        # print(\"  Peringatan LSTM: Vektor input untuk LSTM adalah nol. Skor LSTM diatur ke 0.\")\n",
        "        pass # skor_lstm_pred_val sudah 0.0\n",
        "    else:\n",
        "        try:\n",
        "            # LSTMManual.predict() mengharapkan satu vektor, bukan list of sequences\n",
        "            pred_output_lstm = lstm_model_param.predict(vektor_jawaban_for_lstm)\n",
        "            skor_lstm_pred_val = pred_output_lstm.item() # Ambil nilai skalar dari output (1,1)\n",
        "        except Exception as e:\n",
        "            print(f\"  Error saat prediksi LSTM manual: {e}. Skor LSTM diatur ke 0.\")\n",
        "            skor_lstm_pred_val = 0.0\n",
        "\n",
        "    # --- Logika Pembobotan Dinamis (Sesuai notebook asli Anda) ---\n",
        "    if skor_rouge_val <= 0.01 and skor_tfidf_val <= 0.01:\n",
        "        bobot_rouge_final_g5 = 0.00\n",
        "        bobot_tfidf_final_g5 = 0.00\n",
        "        bobot_cosine_final_g5 = 0.40\n",
        "        bobot_lstm_final_g5 = 0.60\n",
        "    else:\n",
        "        bobot_rouge_final_g5 = 0.05\n",
        "        bobot_tfidf_final_g5 = 0.05\n",
        "        bobot_cosine_final_g5 = 0.25\n",
        "        bobot_lstm_final_g5 = 0.65\n",
        "\n",
        "    skor_akhir_gabungan_g5 = (skor_rouge_val * bobot_rouge_final_g5) + \\\n",
        "                             (skor_tfidf_val * bobot_tfidf_final_g5) + \\\n",
        "                             (skor_cosine_embedding_val * bobot_cosine_final_g5) + \\\n",
        "                             (skor_lstm_pred_val * bobot_lstm_final_g5)\n",
        "    skor_akhir_gabungan_g5 = max(0.0, min(1.0, skor_akhir_gabungan_g5))\n",
        "    skor_sebelum_scaling_g5 = skor_akhir_gabungan_g5\n",
        "\n",
        "    # --- STRATEGI SCALING NON-LINEAR DENGAN DUA KEMUNGKINAN THRESHOLD ---\n",
        "    skor_setelah_scaling_g5 = skor_akhir_gabungan_g5\n",
        "    threshold_scaling_menengah_g5 = 0.55\n",
        "    faktor_angkat_menengah_g5 = 0.20\n",
        "    threshold_scaling_tinggi_g5 = 0.62\n",
        "    faktor_angkat_tinggi_g5 = 0.40\n",
        "\n",
        "    scaling_info = \"\"\n",
        "    if skor_akhir_gabungan_g5 > threshold_scaling_tinggi_g5:\n",
        "        sisa_jarak_ke_satu_g5 = 1.0 - skor_akhir_gabungan_g5\n",
        "        penambahan_skor_g5 = sisa_jarak_ke_satu_g5 * faktor_angkat_tinggi_g5\n",
        "        skor_setelah_scaling_g5 = skor_akhir_gabungan_g5 + penambahan_skor_g5\n",
        "        scaling_info = f\"Info Scaling Tinggi: Skor Gab. Awal={skor_sebelum_scaling_g5:.4f} > Thresh={threshold_scaling_tinggi_g5:.2f}. Dinaikkan (Faktor: {faktor_angkat_tinggi_g5:.2f})\"\n",
        "    elif skor_akhir_gabungan_g5 > threshold_scaling_menengah_g5:\n",
        "        sisa_jarak_ke_satu_g5 = 1.0 - skor_akhir_gabungan_g5\n",
        "        penambahan_skor_g5 = sisa_jarak_ke_satu_g5 * faktor_angkat_menengah_g5\n",
        "        skor_setelah_scaling_g5 = skor_akhir_gabungan_g5 + penambahan_skor_g5\n",
        "        scaling_info = f\"Info Scaling Menengah: Skor Gab. Awal={skor_sebelum_scaling_g5:.4f} > Thresh={threshold_scaling_menengah_g5:.2f}. Dinaikkan (Faktor: {faktor_angkat_menengah_g5:.2f})\"\n",
        "\n",
        "    skor_akhir_final_g5 = max(0.0, min(1.0, skor_setelah_scaling_g5))\n",
        "\n",
        "    # Cetak detail skor dan info scaling jika ada\n",
        "    if scaling_info:\n",
        "        print(f\"    {scaling_info}\")\n",
        "    print(f\"    Detail Skor: ROUGE={skor_rouge_val:.4f} (B:{bobot_rouge_final_g5:.2f}), \"\n",
        "          f\"TF-IDF={skor_tfidf_val:.4f} (B:{bobot_tfidf_final_g5:.2f}), \"\n",
        "          f\"CosEmb={skor_cosine_embedding_val:.4f} (B:{bobot_cosine_final_g5:.2f}), \"\n",
        "          f\"LSTM={skor_lstm_pred_val:.4f} (B:{bobot_lstm_final_g5:.2f}) -> Gabungan={skor_sebelum_scaling_g5:.4f} -> Scaled Akhir={skor_akhir_final_g5:.4f}\")\n",
        "\n",
        "    return float(skor_akhir_final_g5)\n",
        "\n",
        "print(\"Fungsi evaluate_answer_g5 (dengan LSTM Manual dan Dua Threshold Scaling) telah didefinisikan.\\n\")"
      ],
      "metadata": {
        "id": "WU5-uOX9pBgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 3: Membaca dan Mem-parsing File input_soal.txt ===\n",
        "print(\"--- GRUP 5: SEL 3 ---\")\n",
        "\n",
        "def parse_input_soal_file_g5(file_path_param):\n",
        "    \"\"\"\n",
        "    Membaca dan mem-parsing file input_soal.txt untuk mendapatkan daftar tugas evaluasi.\n",
        "    \"\"\"\n",
        "    tasks_parsed = []\n",
        "    # Pastikan file_path_param adalah string dan file ada\n",
        "    if not isinstance(file_path_param, str) or not os.path.exists(file_path_param):\n",
        "        print(f\"  Error: Path file input '{file_path_param}' tidak valid atau file tidak ditemukan.\")\n",
        "        return tasks_parsed\n",
        "\n",
        "    try:\n",
        "        with open(file_path_param, 'r', encoding='utf-8') as f_input:\n",
        "            content_input = f_input.read()\n",
        "    except Exception as e:\n",
        "        print(f\"  Error saat membaca file {file_path_param}: {e}\")\n",
        "        return tasks_parsed\n",
        "\n",
        "    # Membersihkan karakter '\\' yang mungkin diikuti spasi (sering muncul dari copy-paste)\n",
        "    content_cleaned_input = re.sub(r'\\\\\\s*', '', content_input)\n",
        "    # Memisahkan blok soal berdasarkan dua atau lebih baris baru\n",
        "    blocks_input = re.split(r'\\n\\s*\\n+', content_cleaned_input.strip())\n",
        "\n",
        "    for i_block, block_text in enumerate(blocks_input):\n",
        "        if not block_text.strip(): continue\n",
        "\n",
        "        current_task_item = {\n",
        "            \"id_soal\": f\"InputFile_{i_block+1:03d}\",\n",
        "            \"pertanyaan\": \"\",\n",
        "            \"jawaban_siswa\": \"\",\n",
        "            \"kata_kunci\": []\n",
        "        }\n",
        "        for line_text in block_text.strip().split('\\n'):\n",
        "            line_text_stripped = line_text.strip()\n",
        "            if line_text_stripped.lower().startswith(\"pertanyaan:\"):\n",
        "                current_task_item[\"pertanyaan\"] = line_text_stripped.split(\":\", 1)[1].strip()\n",
        "            elif line_text_stripped.lower().startswith(\"jawaban:\"):\n",
        "                current_task_item[\"jawaban_siswa\"] = line_text_stripped.split(\":\", 1)[1].strip()\n",
        "            elif line_text_stripped.lower().startswith(\"kata kunci:\"):\n",
        "                keywords_str_input = line_text_stripped.split(\":\", 1)[1].strip()\n",
        "                current_task_item[\"kata_kunci\"] = [kw.strip() for kw in keywords_str_input.split(',') if kw.strip()]\n",
        "\n",
        "        if current_task_item[\"pertanyaan\"] and current_task_item[\"jawaban_siswa\"]:\n",
        "            tasks_parsed.append(current_task_item)\n",
        "        else:\n",
        "            print(f\"  Peringatan: Blok soal ke-{i_block+1} di file '{os.path.basename(file_path_param)}' tidak memiliki 'Pertanyaan:' atau 'Jawaban:' yang valid. Dilewati.\")\n",
        "\n",
        "    return tasks_parsed\n",
        "\n",
        "# Path dari Grup 1: input_soal_txt_path_global\n",
        "print(f\"Membaca dan mem-parsing file input: {input_soal_txt_path_global}\")\n",
        "tasks_to_evaluate_final_g5 = parse_input_soal_file_g5(input_soal_txt_path_global)\n",
        "\n",
        "if not tasks_to_evaluate_final_g5:\n",
        "    print(\"Tidak ada tugas evaluasi yang berhasil diparsing. Periksa file input atau path.\")\n",
        "else:\n",
        "    print(f\"Berhasil mem-parsing {len(tasks_to_evaluate_final_g5)} tugas evaluasi dari '{os.path.basename(input_soal_txt_path_global)}'.\")\n",
        "    if tasks_to_evaluate_final_g5:\n",
        "        print(\"Contoh tugas pertama yang diparsing:\")\n",
        "        # Cetak dengan json.dumps untuk format yang lebih rapi, pastikan ensure_ascii=False untuk karakter non-ASCII\n",
        "        print(json.dumps(tasks_to_evaluate_final_g5[0], indent=2, ensure_ascii=False))\n",
        "print(\"Parsing file input selesai.\\n\")"
      ],
      "metadata": {
        "id": "XE347xPEpDMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell 4: Loop Evaluasi dari Data yang Diparsing dan Penyimpanan Hasil ===\n",
        "print(\"--- GRUP 5: SEL 4 ---\")\n",
        "hasil_penilaian_akhir_g5 = []\n",
        "\n",
        "if not komponen_evaluasi_siap:\n",
        "    print(\"Evaluasi tidak dapat dilanjutkan karena komponen penting tidak siap.\")\n",
        "elif not tasks_to_evaluate_final_g5:\n",
        "    print(\"Tidak ada tugas untuk dievaluasi (data parsing dari file .txt gagal atau file kosong).\")\n",
        "else:\n",
        "    print(\"Memulai Proses Evaluasi Jawaban dari file input...\")\n",
        "    for i_task, task_item_eval in enumerate(tasks_to_evaluate_final_g5):\n",
        "        print(f\"\\nMengevaluasi Tugas ID: {task_item_eval.get('id_soal', f'Index_{i_task+1}')}\")\n",
        "\n",
        "        soal_eval_g5 = task_item_eval.get(\"pertanyaan\", \"\")\n",
        "        jawaban_siswa_eval_g5 = task_item_eval.get(\"jawaban_siswa\", \"\")\n",
        "        kata_kunci_eval_g5 = task_item_eval.get(\"kata_kunci\", [])\n",
        "\n",
        "        print(f\"  Soal (awal): {soal_eval_g5[:80]}...\") # Tampilkan 80 karakter pertama soal\n",
        "\n",
        "        skor_final_eval_val = 0.0\n",
        "        if not jawaban_siswa_eval_g5.strip():\n",
        "            print(\"  Peringatan: Jawaban siswa untuk tugas ini kosong. Skor diatur ke 0.\")\n",
        "        else:\n",
        "            # Panggil fungsi evaluate_answer_g5\n",
        "            # Model untuk cosine sim adalah model besar pra-latih\n",
        "            # Model untuk input LSTM adalah model fine-tuned (ft_model_global)\n",
        "            skor_final_eval_val = evaluate_answer_g5(\n",
        "                jawaban_siswa_text_param=jawaban_siswa_eval_g5,\n",
        "                soal_text_param=soal_eval_g5,\n",
        "                kata_kunci_list_param=kata_kunci_eval_g5,\n",
        "                embedding_model_for_cosine_param=embedding_model_large_vocab_global,\n",
        "                mean_vector_for_cosine_param=mean_vector_for_large_vocab_global,\n",
        "                embedding_model_for_lstm_input=ft_model_global,\n",
        "                mean_vector_for_lstm_input=mean_fasttext_vector_global,\n",
        "                lstm_model_param=lstm_manual_model_instance_global\n",
        "                # konteks_tfidf_dokumen bisa ditambahkan jika ada corpus yang lebih besar untuk IDF\n",
        "            )\n",
        "\n",
        "        hasil_penilaian_akhir_g5.append({\n",
        "            \"id_soal\": task_item_eval.get(\"id_soal\", f\"Index_{i_task+1}\"),\n",
        "            \"soal\": soal_eval_g5,\n",
        "            \"jawaban_siswa_input\": jawaban_siswa_eval_g5,\n",
        "            \"kata_kunci_input\": kata_kunci_eval_g5,\n",
        "            \"skor_total_numerik\": skor_final_eval_val,\n",
        "            \"skor_total_persen\": f\"{skor_final_eval_val * 100:.2f}\"\n",
        "        })\n",
        "        print(f\"  Skor Total Gabungan untuk Tugas '{task_item_eval.get('id_soal', '')}': {skor_final_eval_val * 100:.2f}/100\")\n",
        "\n",
        "    # --- Cetak dan Simpan Hasil Penilaian Akhir ---\n",
        "    print(\"\\n\\n===== HASIL PENILAIAN AKHIR (dari input_soal.txt) =====\")\n",
        "    if not hasil_penilaian_akhir_g5:\n",
        "        print(\"Tidak ada hasil penilaian untuk ditampilkan.\")\n",
        "    else:\n",
        "        try:\n",
        "            # Menggunakan Pandas untuk tampilan tabel jika tersedia dan diizinkan\n",
        "            df_hasil_g5 = pd.DataFrame(hasil_penilaian_akhir_g5)\n",
        "            # Tampilkan kolom yang relevan, sesuaikan dengan permintaan output Anda\n",
        "            print(df_hasil_g5[['id_soal', 'soal', 'skor_total_persen']].to_string(index=False))\n",
        "        except Exception as e:\n",
        "            print(f\"Error membuat DataFrame dengan Pandas: {e}. Mencetak hasil secara manual:\")\n",
        "            for hasil_item_print in hasil_penilaian_akhir_g5:\n",
        "                print(f\"\\nID Tugas: {hasil_item_print['id_soal']}\")\n",
        "                print(f\"Soal: {hasil_item_print['soal']}\")\n",
        "                print(f\"Kata Kunci: {', '.join(hasil_item_print['kata_kunci_input']) if hasil_item_print['kata_kunci_input'] else '-'}\")\n",
        "                print(f\"Skor Total: {hasil_item_print['skor_total_persen']}/100\")\n",
        "                print(\"-\" * 40)\n",
        "\n",
        "    # Path dari Grup 1: hasil_evaluasi_akhir_file_global\n",
        "    if hasil_penilaian_akhir_g5: # Hanya simpan jika ada hasil\n",
        "        try:\n",
        "            with open(hasil_evaluasi_akhir_file_global, 'w', encoding='utf-8') as f_out_eval:\n",
        "               json.dump(hasil_penilaian_akhir_g5, f_out_eval, indent=2, ensure_ascii=False)\n",
        "            print(f\"\\nHasil penilaian akhir juga disimpan dalam format JSON di: {hasil_evaluasi_akhir_file_global}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Gagal menyimpan hasil penilaian akhir ke JSON: {e}\")\n",
        "\n",
        "print(\"\\n--- Grup 5 Selesai ---\")"
      ],
      "metadata": {
        "id": "KrtquhNZpFNe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}